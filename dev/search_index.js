var documenterSearchIndex = {"docs":
[{"location":"manual/ode/#ODE-Specialized-Physics-Informed-Neural-Network-(PINN)-Solver","page":"ODE-Specialized Physics-Informed Neural Network (PINN) Solver","title":"ODE-Specialized Physics-Informed Neural Network (PINN) Solver","text":"","category":"section"},{"location":"manual/ode/","page":"ODE-Specialized Physics-Informed Neural Network (PINN) Solver","title":"ODE-Specialized Physics-Informed Neural Network (PINN) Solver","text":"NNODE","category":"page"},{"location":"manual/ode/#NeuralPDE.NNODE","page":"ODE-Specialized Physics-Informed Neural Network (PINN) Solver","title":"NeuralPDE.NNODE","text":"NNODE(chain, opt=OptimizationPolyalgorithms.PolyOpt(), init_params = nothing;\n                          autodiff=false, batch=0,additional_loss=nothing,\n                          kwargs...)\n\nAlgorithm for solving ordinary differential equations using a neural network. This is a specialization of the physics-informed neural network which is used as a solver for a standard ODEProblem.\n\nwarn: Warn\nNote that NNODE only supports ODEs which are written in the out-of-place form, i.e. du = f(u,p,t), and not f(du,u,p,t). If not declared out-of-place, then the NNODE will exit with an error.\n\nPositional Arguments\n\nchain: A neural network architecture, defined as either a Flux.Chain or a Lux.AbstractExplicitLayer.\nopt: The optimizer to train the neural network. Defaults to OptimizationPolyalgorithms.PolyOpt()\ninit_params: The initial parameter of the neural network. By default, this is nothing which thus uses the random initialization provided by the neural network library.\n\nKeyword Arguments\n\nadditional_loss: A function additional_loss(phi, θ) where phi are the neural network trial solutions,                    θ are the weights of the neural network(s).\n\nExample\n\nu0 = [1.0, 1.0]\n    ts=[t for t in 1:100]\n    (u_, t_) = (analytical_func(ts), ts)\n    function additional_loss(phi, θ)\n        return sum(sum(abs2, [phi(t, θ) for t in t_] .- u_)) / length(u_)\n    end\n    alg = NeuralPDE.NNODE(chain, opt, additional_loss = additional_loss)\n\nautodiff: The switch between automatic and numerical differentiation for             the PDE operators. The reverse mode of the loss function is always             automatic differentiation (via Zygote), this is only for the derivative             in the loss function (the derivative with respect to time).\nbatch: The batch size to use for the internal quadrature. Defaults to 0, which means the application of the neural network is done at individual time points one at a time. batch>0 means the neural network is applied at a row vector of values t simultaneously, i.e. it's the batch size for the neural network evaluations. This requires a neural network compatible with batched data.\nstrategy: The training strategy used to choose the points for the evaluations. Default of nothing means that QuadratureTraining with QuadGK is used if no dt is given, and GridTraining is used with dt if given.\nkwargs: Extra keyword arguments are splatted to the Optimization.jl solve call.\n\nExample\n\nf(u,p,t) = cos(2pi*t)\ntspan = (0.0f0, 1.0f0)\nu0 = 0.0f0\nprob = ODEProblem(linear, u0 ,tspan)\nchain = Flux.Chain(Dense(1,5,σ),Dense(5,1))\nopt = Flux.ADAM(0.1)\nsol = solve(prob, NeuralPDE.NNODE(chain,opt), dt=1/20f0, verbose = true,\n            abstol=1e-10, maxiters = 200)\n\nSolution Notes\n\nNote that the solution is evaluated at fixed time points according to standard output handlers such as saveat and dt. However, the neural network is a fully continuous solution so sol(t) is an accurate interpolation (up to the neural network training result). In addition, the OptimizationSolution is returned as sol.k for further analysis.\n\nReferences\n\nLagaris, Isaac E., Aristidis Likas, and Dimitrios I. Fotiadis. \"Artificial neural networks for solving ordinary and partial differential equations.\" IEEE Transactions on Neural Networks 9, no. 5 (1998): 987-1000.\n\n\n\n\n\n","category":"type"},{"location":"tutorials/low_level/#Investigating-symbolic_discretize-with-the-1-D-Burgers'-Equation","page":"The symbolic_discretize Interface","title":"Investigating symbolic_discretize with the 1-D Burgers' Equation","text":"","category":"section"},{"location":"tutorials/low_level/","page":"The symbolic_discretize Interface","title":"The symbolic_discretize Interface","text":"Let's consider the Burgers' equation:","category":"page"},{"location":"tutorials/low_level/","page":"The symbolic_discretize Interface","title":"The symbolic_discretize Interface","text":"begingather*\n_t u + u _x u - (001  pi) _x^2 u = 0   quad x in -1 1 t in 0 1   \nu(0 x) = - sin(pi x)   \nu(t -1) = u(t 1) = 0  \nendgather*","category":"page"},{"location":"tutorials/low_level/","page":"The symbolic_discretize Interface","title":"The symbolic_discretize Interface","text":"with Physics-Informed Neural Networks. Here is an example of using the low-level API:","category":"page"},{"location":"tutorials/low_level/","page":"The symbolic_discretize Interface","title":"The symbolic_discretize Interface","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\n@variables u(..)\nDt = Differential(t)\nDx = Differential(x)\nDxx = Differential(x)^2\n\n#2D PDE\neq = Dt(u(t, x)) + u(t, x) * Dx(u(t, x)) - (0.01 / pi) * Dxx(u(t, x)) ~ 0\n\n# Initial and boundary conditions\nbcs = [u(0, x) ~ -sin(pi * x),\n    u(t, -1) ~ 0.0,\n    u(t, 1) ~ 0.0,\n    u(t, -1) ~ u(t, 1)]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0, 1.0),\n    x ∈ Interval(-1.0, 1.0)]\n\n# Discretization\ndx = 0.05\n\n# Neural network\nchain = Lux.Chain(Dense(2, 16, Lux.σ), Dense(16, 16, Lux.σ), Dense(16, 1))\nstrategy = NeuralPDE.GridTraining(dx)\n\nindvars = [t, x]\ndepvars = [u(t, x)]\n@named pde_system = PDESystem(eq, bcs, domains, indvars, depvars)\n\ndiscretization = PhysicsInformedNN(chain, strategy)\nsym_prob = symbolic_discretize(pde_system, discretization)\n\nphi = sym_prob.phi\n\npde_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbc_loss_functions = sym_prob.loss_functions.bc_loss_functions\n\ncallback = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p), pde_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p), bc_loss_functions))\n    return false\nend\n\nloss_functions = [pde_loss_functions; bc_loss_functions]\n\nfunction loss_function(θ, p)\n    sum(map(l -> l(θ), loss_functions))\nend\n\nf_ = OptimizationFunction(loss_function, Optimization.AutoZygote())\nprob = Optimization.OptimizationProblem(f_, sym_prob.flat_init_params)\n\nres = Optimization.solve(prob, OptimizationOptimJL.BFGS(); callback = callback,\n                         maxiters = 2000)","category":"page"},{"location":"tutorials/low_level/","page":"The symbolic_discretize Interface","title":"The symbolic_discretize Interface","text":"And some analysis:","category":"page"},{"location":"tutorials/low_level/","page":"The symbolic_discretize Interface","title":"The symbolic_discretize Interface","text":"using Plots\n\nts, xs = [infimum(d.domain):dx:supremum(d.domain) for d in domains]\nu_predict_contourf = reshape([first(phi([t, x], res.u)) for t in ts for x in xs],\n                             length(xs), length(ts))\nplot(ts, xs, u_predict_contourf, linetype = :contourf, title = \"predict\")\n\nu_predict = [[first(phi([t, x], res.u)) for x in xs] for t in ts]\np1 = plot(xs, u_predict[3], title = \"t = 0.1\");\np2 = plot(xs, u_predict[11], title = \"t = 0.5\");\np3 = plot(xs, u_predict[end], title = \"t = 1\");\nplot(p1, p2, p3)","category":"page"},{"location":"tutorials/low_level/","page":"The symbolic_discretize Interface","title":"The symbolic_discretize Interface","text":"(Image: burgers)","category":"page"},{"location":"tutorials/low_level/","page":"The symbolic_discretize Interface","title":"The symbolic_discretize Interface","text":"(Image: burgers2)","category":"page"},{"location":"examples/linear_parabolic/#Linear-parabolic-system-of-PDEs","page":"Linear parabolic system of PDEs","title":"Linear parabolic system of PDEs","text":"","category":"section"},{"location":"examples/linear_parabolic/","page":"Linear parabolic system of PDEs","title":"Linear parabolic system of PDEs","text":"We can use NeuralPDE to solve the linear parabolic system of PDEs:","category":"page"},{"location":"examples/linear_parabolic/","page":"Linear parabolic system of PDEs","title":"Linear parabolic system of PDEs","text":"beginaligned\nfracpartial upartial t = a * fracpartial^2 upartial x^2 + b_1 u + c_1 w \nfracpartial wpartial t = a * fracpartial^2 wpartial x^2 + b_2 u + c_2 w \nendaligned","category":"page"},{"location":"examples/linear_parabolic/","page":"Linear parabolic system of PDEs","title":"Linear parabolic system of PDEs","text":"with initial and boundary conditions:","category":"page"},{"location":"examples/linear_parabolic/","page":"Linear parabolic system of PDEs","title":"Linear parabolic system of PDEs","text":"beginaligned\nu(0 x) = fracb_1 - lambda_2b_2 (lambda_1 - lambda_2) cdot cos(fracxa) -  fracb_1 - lambda_1b_2 (lambda_1 - lambda_2) cdot cos(fracxa) \nw(0 x) = 0 \nu(t 0) = fracb_1 - lambda_2b_2 (lambda_1 - lambda_2) cdot e^lambda_1t -  fracb_1 - lambda_1b_2 (lambda_1 - lambda_2) cdot e^lambda_2t  w(t 0) = frace^lambda_1-e^lambda_2lambda_1 - lambda_2 \nu(t 1) = fracb_1 - lambda_2b_2 (lambda_1 - lambda_2) cdot e^lambda_1t cdot cos(fracxa) -  fracb_1 - lambda_1b_2 (lambda_1 - lambda_2) cdot e^lambda_2t * cos(fracxa) \nw(t 1) = frace^lambda_1 cos(fracxa)-e^lambda_2cos(fracxa)lambda_1 - lambda_2\nendaligned","category":"page"},{"location":"examples/linear_parabolic/","page":"Linear parabolic system of PDEs","title":"Linear parabolic system of PDEs","text":"with a physics-informed neural network.","category":"page"},{"location":"examples/linear_parabolic/","page":"Linear parabolic system of PDEs","title":"Linear parabolic system of PDEs","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL\nusing Plots\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\n@variables u(..), w(..)\nDxx = Differential(x)^2\nDt = Differential(t)\n\n# Constants\na = 1\nb1 = 4\nb2 = 2\nc1 = 3\nc2 = 1\nλ1 = (b1 + c2 + sqrt((b1 + c2)^2 + 4 * (b1 * c2 - b2 * c1))) / 2\nλ2 = (b1 + c2 - sqrt((b1 + c2)^2 + 4 * (b1 * c2 - b2 * c1))) / 2\n\n# Analytic solution\nθ(t, x) = exp(-t) * cos(x / a)\nfunction u_analytic(t, x)\n    (b1 - λ2) / (b2 * (λ1 - λ2)) * exp(λ1 * t) * θ(t, x) -\n    (b1 - λ1) / (b2 * (λ1 - λ2)) * exp(λ2 * t) * θ(t, x)\nend\nw_analytic(t, x) = 1 / (λ1 - λ2) * (exp(λ1 * t) * θ(t, x) - exp(λ2 * t) * θ(t, x))\n\n# Second-order constant-coefficient linear parabolic system\neqs = [Dt(u(x, t)) ~ a * Dxx(u(x, t)) + b1 * u(x, t) + c1 * w(x, t),\n    Dt(w(x, t)) ~ a * Dxx(w(x, t)) + b2 * u(x, t) + c2 * w(x, t)]\n\n# Boundary conditions\nbcs = [u(0, x) ~ u_analytic(0, x),\n    w(0, x) ~ w_analytic(0, x),\n    u(t, 0) ~ u_analytic(t, 0),\n    w(t, 0) ~ w_analytic(t, 0),\n    u(t, 1) ~ u_analytic(t, 1),\n    w(t, 1) ~ w_analytic(t, 1)]\n\n# Space and time domains\ndomains = [x ∈ Interval(0.0, 1.0),\n    t ∈ Interval(0.0, 1.0)]\n\n# Neural network\ninput_ = length(domains)\nn = 15\nchain = [Lux.Chain(Dense(input_, n, Lux.σ), Dense(n, n, Lux.σ), Dense(n, 1)) for _ in 1:2]\n\nstrategy = QuadratureTraining()\ndiscretization = PhysicsInformedNN(chain, strategy)\n\n@named pdesystem = PDESystem(eqs, bcs, domains, [t, x], [u(t, x), w(t, x)])\nprob = discretize(pdesystem, discretization)\nsym_prob = symbolic_discretize(pdesystem, discretization)\n\npde_inner_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbcs_inner_loss_functions = sym_prob.loss_functions.bc_loss_functions\n\ncallback = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p), bcs_inner_loss_functions))\n    return false\nend\n\nres = Optimization.solve(prob, BFGS(); callback = callback, maxiters = 5000)\n\nphi = discretization.phi\n\n# Analysis\nts, xs = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\ndepvars = [:u, :w]\nminimizers_ = [res.u.depvar[depvars[i]] for i in 1:length(chain)]\n\nanalytic_sol_func(t, x) = [u_analytic(t, x), w_analytic(t, x)]\nu_real = [[analytic_sol_func(t, x)[i] for t in ts for x in xs] for i in 1:2]\nu_predict = [[phi[i]([t, x], minimizers_[i])[1] for t in ts for x in xs] for i in 1:2]\ndiff_u = [abs.(u_real[i] .- u_predict[i]) for i in 1:2]\nfor i in 1:2\n    p1 = plot(ts, xs, u_real[i], linetype = :contourf, title = \"u$i, analytic\")\n    p2 = plot(ts, xs, u_predict[i], linetype = :contourf, title = \"predict\")\n    p3 = plot(ts, xs, diff_u[i], linetype = :contourf, title = \"error\")\n    plot(p1, p2, p3)\n    savefig(\"sol_u$i\")\nend","category":"page"},{"location":"examples/linear_parabolic/","page":"Linear parabolic system of PDEs","title":"Linear parabolic system of PDEs","text":"(Image: linear_parabolic_sol_u1) (Image: linear_parabolic_sol_u2)","category":"page"},{"location":"tutorials/gpu/#Using-GPUs-to-train-Physics-Informed-Neural-Networks-(PINNs)","page":"Using GPUs","title":"Using GPUs to train Physics-Informed Neural Networks (PINNs)","text":"","category":"section"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"the 2-dimensional PDE:","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"_t u(x y t) = ^2_x u(x y t) + ^2_y u(x y t)  ","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"with the initial and boundary conditions:","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"beginalign*\nu(x y 0) = e^x+y cos(x + y)       \nu(0 y t) = e^y   cos(y + 4t)      \nu(2 y t) = e^2+y cos(2 + y + 4t)  \nu(x 0 t) = e^x   cos(x + 4t)      \nu(x 2 t) = e^x+2 cos(x + 2 + 4t)  \nendalign*","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"on the space and time domain:","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"x in 0 2   y in 0 2    t in 0 2  ","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"with physics-informed neural networks. The only major difference from the CPU case is that we must ensure that our initial parameters for the neural network are on the GPU. If that is done, then the internal computations will all take place on the GPU. This is done by using the gpu function on the initial parameters, like:","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"using Lux, ComponentArrays\nchain = Chain(Dense(3, inner, Lux.σ),\n              Dense(inner, inner, Lux.σ),\n              Dense(inner, inner, Lux.σ),\n              Dense(inner, inner, Lux.σ),\n              Dense(inner, 1))\nps = Lux.setup(Random.default_rng(), chain)[1]\nps = ps |> ComponentArray |> gpu .|> Float64","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"In total, this looks like:","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"using NeuralPDE, Lux, CUDA, Random, ComponentArrays\nusing Optimization\nusing OptimizationOptimisers\nimport ModelingToolkit: Interval\n\n@parameters t x y\n@variables u(..)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\nDt = Differential(t)\nt_min = 0.0\nt_max = 2.0\nx_min = 0.0\nx_max = 2.0\ny_min = 0.0\ny_max = 2.0\n\n# 2D PDE\neq = Dt(u(t, x, y)) ~ Dxx(u(t, x, y)) + Dyy(u(t, x, y))\n\nanalytic_sol_func(t, x, y) = exp(x + y) * cos(x + y + 4t)\n# Initial and boundary conditions\nbcs = [u(t_min, x, y) ~ analytic_sol_func(t_min, x, y),\n    u(t, x_min, y) ~ analytic_sol_func(t, x_min, y),\n    u(t, x_max, y) ~ analytic_sol_func(t, x_max, y),\n    u(t, x, y_min) ~ analytic_sol_func(t, x, y_min),\n    u(t, x, y_max) ~ analytic_sol_func(t, x, y_max)]\n\n# Space and time domains\ndomains = [t ∈ Interval(t_min, t_max),\n    x ∈ Interval(x_min, x_max),\n    y ∈ Interval(y_min, y_max)]\n\n# Neural network\ninner = 25\nchain = Chain(Dense(3, inner, Lux.σ),\n              Dense(inner, inner, Lux.σ),\n              Dense(inner, inner, Lux.σ),\n              Dense(inner, inner, Lux.σ),\n              Dense(inner, 1))\n\nstrategy = GridTraining(0.05)\nps = Lux.setup(Random.default_rng(), chain)[1]\nps = ps |> ComponentArray |> gpu .|> Float64\ndiscretization = PhysicsInformedNN(chain,\n                                   strategy,\n                                   init_params = ps)\n\n@named pde_system = PDESystem(eq, bcs, domains, [t, x, y], [u(t, x, y)])\nprob = discretize(pde_system, discretization)\nsymprob = symbolic_discretize(pde_system, discretization)\n\ncallback = function (p, l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nres = Optimization.solve(prob, Adam(0.01); callback = callback, maxiters = 2500)","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"We then use the remake function to rebuild the PDE problem to start a new optimization at the optimized parameters, and continue with a lower learning rate:","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"prob = remake(prob, u0 = res.u)\nres = Optimization.solve(prob, Adam(0.001); callback = callback, maxiters = 2500)","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"Finally, we inspect the solution:","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"phi = discretization.phi\nts, xs, ys = [infimum(d.domain):0.1:supremum(d.domain) for d in domains]\nu_real = [analytic_sol_func(t, x, y) for t in ts for x in xs for y in ys]\nu_predict = [first(Array(phi(gpu([t, x, y]), res.u))) for t in ts for x in xs for y in ys]\n\nusing Plots\nusing Printf\n\nfunction plot_(res)\n    # Animate\n    anim = @animate for (i, t) in enumerate(0:0.05:t_max)\n        @info \"Animating frame $i...\"\n        u_real = reshape([analytic_sol_func(t, x, y) for x in xs for y in ys],\n                         (length(xs), length(ys)))\n        u_predict = reshape([Array(phi(gpu([t, x, y]), res.u))[1] for x in xs for y in ys],\n                            length(xs), length(ys))\n        u_error = abs.(u_predict .- u_real)\n        title = @sprintf(\"predict, t = %.3f\", t)\n        p1 = plot(xs, ys, u_predict, st = :surface, label = \"\", title = title)\n        title = @sprintf(\"real\")\n        p2 = plot(xs, ys, u_real, st = :surface, label = \"\", title = title)\n        title = @sprintf(\"error\")\n        p3 = plot(xs, ys, u_error, st = :contourf, label = \"\", title = title)\n        plot(p1, p2, p3)\n    end\n    gif(anim, \"3pde.gif\", fps = 10)\nend\n\nplot_(res)","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"(Image: 3pde)","category":"page"},{"location":"tutorials/gpu/#Performance-benchmarks","page":"Using GPUs","title":"Performance benchmarks","text":"","category":"section"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"Here are some performance benchmarks for 2d-pde with various number of input points and the number of neurons in the hidden layer, measuring the time for 100 iterations. Comparing runtime with GPU and CPU.","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"julia> CUDA.device()\n","category":"page"},{"location":"tutorials/gpu/","page":"Using GPUs","title":"Using GPUs","text":"(Image: image)","category":"page"},{"location":"tutorials/systems/#systems","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs for Physics-Informed Neural Networks (PINNs)","text":"","category":"section"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"In this example, we will solve the PDE system:","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"beginalign*\n_t^2 u_1(t x)  = _x^2 u_1(t x) + u_3(t x)  sin(pi x)  \n_t^2 u_2(t x)  = _x^2 u_2(t x) + u_3(t x)  cos(pi x)  \n0  = u_1(t x) sin(pi x) + u_2(t x) cos(pi x) - e^-t  \nendalign*","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"with the initial conditions:","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"beginalign*\nu_1(0 x)  = sin(pi x)  \n_t u_1(0 x)  = - sin(pi x)  \nu_2(0 x)  = cos(pi x)  \n_t u_2(0 x)  = - cos(pi x)  \nendalign*","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"and the boundary conditions:","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"beginalign*\nu_1(t 0)  = u_1(t 1) = 0  \nu_2(t 0)  = - u_2(t 1) = e^-t  \nendalign*","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"with physics-informed neural networks.","category":"page"},{"location":"tutorials/systems/#Solution","page":"Defining Systems of PDEs","title":"Solution","text":"","category":"section"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\n@variables u1(..), u2(..), u3(..)\nDt = Differential(t)\nDtt = Differential(t)^2\nDx = Differential(x)\nDxx = Differential(x)^2\n\neqs = [Dtt(u1(t, x)) ~ Dxx(u1(t, x)) + u3(t, x) * sin(pi * x),\n    Dtt(u2(t, x)) ~ Dxx(u2(t, x)) + u3(t, x) * cos(pi * x),\n    0.0 ~ u1(t, x) * sin(pi * x) + u2(t, x) * cos(pi * x) - exp(-t)]\n\nbcs = [u1(0, x) ~ sin(pi * x),\n    u2(0, x) ~ cos(pi * x),\n    Dt(u1(0, x)) ~ -sin(pi * x),\n    Dt(u2(0, x)) ~ -cos(pi * x),\n    u1(t, 0) ~ 0.0,\n    u2(t, 0) ~ exp(-t),\n    u1(t, 1) ~ 0.0,\n    u2(t, 1) ~ -exp(-t)]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0, 1.0),\n    x ∈ Interval(0.0, 1.0)]\n\n# Neural network\ninput_ = length(domains)\nn = 15\nchain = [Lux.Chain(Dense(input_, n, Lux.σ), Dense(n, n, Lux.σ), Dense(n, 1)) for _ in 1:3]\n\nstrategy = QuadratureTraining()\ndiscretization = PhysicsInformedNN(chain, strategy)\n\n@named pdesystem = PDESystem(eqs, bcs, domains, [t, x], [u1(t, x), u2(t, x), u3(t, x)])\nprob = discretize(pdesystem, discretization)\nsym_prob = symbolic_discretize(pdesystem, discretization)\n\npde_inner_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbcs_inner_loss_functions = sym_prob.loss_functions.bc_loss_functions\n\ncallback = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p), bcs_inner_loss_functions))\n    return false\nend\n\nres = Optimization.solve(prob, BFGS(); callback = callback, maxiters = 5000)\n\nphi = discretization.phi","category":"page"},{"location":"tutorials/systems/#Direct-Construction-via-symbolic_discretize","page":"Defining Systems of PDEs","title":"Direct Construction via symbolic_discretize","text":"","category":"section"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"One can take apart the pieces and reassemble the loss functions using the symbolic_discretize interface. Here is an example using the components from symbolic_discretize to fully reproduce the discretize optimization:","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\n@variables u1(..), u2(..), u3(..)\nDt = Differential(t)\nDtt = Differential(t)^2\nDx = Differential(x)\nDxx = Differential(x)^2\n\neqs = [Dtt(u1(t, x)) ~ Dxx(u1(t, x)) + u3(t, x) * sin(pi * x),\n    Dtt(u2(t, x)) ~ Dxx(u2(t, x)) + u3(t, x) * cos(pi * x),\n    0.0 ~ u1(t, x) * sin(pi * x) + u2(t, x) * cos(pi * x) - exp(-t)]\n\nbcs = [u1(0, x) ~ sin(pi * x),\n    u2(0, x) ~ cos(pi * x),\n    Dt(u1(0, x)) ~ -sin(pi * x),\n    Dt(u2(0, x)) ~ -cos(pi * x),\n    u1(t, 0) ~ 0.0,\n    u2(t, 0) ~ exp(-t),\n    u1(t, 1) ~ 0.0,\n    u2(t, 1) ~ -exp(-t)]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0, 1.0),\n    x ∈ Interval(0.0, 1.0)]\n\n# Neural network\ninput_ = length(domains)\nn = 15\nchain = [Lux.Chain(Dense(input_, n, Lux.σ), Dense(n, n, Lux.σ), Dense(n, 1)) for _ in 1:3]\n@named pdesystem = PDESystem(eqs, bcs, domains, [t, x], [u1(t, x), u2(t, x), u3(t, x)])\n\nstrategy = NeuralPDE.QuadratureTraining()\ndiscretization = PhysicsInformedNN(chain, strategy)\nsym_prob = NeuralPDE.symbolic_discretize(pdesystem, discretization)\n\npde_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbc_loss_functions = sym_prob.loss_functions.bc_loss_functions\n\ncallback = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p), pde_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p), bc_loss_functions))\n    return false\nend\n\nloss_functions = [pde_loss_functions; bc_loss_functions]\n\nfunction loss_function(θ, p)\n    sum(map(l -> l(θ), loss_functions))\nend\n\nf_ = OptimizationFunction(loss_function, Optimization.AutoZygote())\nprob = Optimization.OptimizationProblem(f_, sym_prob.flat_init_params)\n\nres = Optimization.solve(prob, OptimizationOptimJL.BFGS(); callback = callback,\n                         maxiters = 5000)","category":"page"},{"location":"tutorials/systems/#Solution-Representation","page":"Defining Systems of PDEs","title":"Solution Representation","text":"","category":"section"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"Now let's perform some analysis for both the symbolic_discretize and discretize APIs:","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"using Plots\n\nphi = discretization.phi\nts, xs = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\n\nminimizers_ = [res.u.depvar[sym_prob.depvars[i]] for i in 1:3]\n\nfunction analytic_sol_func(t, x)\n    [exp(-t) * sin(pi * x), exp(-t) * cos(pi * x), (1 + pi^2) * exp(-t)]\nend\nu_real = [[analytic_sol_func(t, x)[i] for t in ts for x in xs] for i in 1:3]\nu_predict = [[phi[i]([t, x], minimizers_[i])[1] for t in ts for x in xs] for i in 1:3]\ndiff_u = [abs.(u_real[i] .- u_predict[i]) for i in 1:3]\nfor i in 1:3\n    p1 = plot(ts, xs, u_real[i], linetype = :contourf, title = \"u$i, analytic\")\n    p2 = plot(ts, xs, u_predict[i], linetype = :contourf, title = \"predict\")\n    p3 = plot(ts, xs, diff_u[i], linetype = :contourf, title = \"error\")\n    plot(p1, p2, p3)\n    savefig(\"sol_u$i\")\nend","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"(Image: sol_uq1)","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"(Image: sol_uq2)","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"(Image: sol_uq3)","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"Notice here that the solution is represented in the OptimizationSolution with u as the parameters for the trained neural network. But, for the case where the neural network is from Lux.jl, it's given as a ComponentArray where res.u.depvar.x corresponds to the result for the neural network corresponding to the dependent variable x, i.e. res.u.depvar.u1 are the trained parameters for phi[1] in our example. For simpler indexing, you can use res.u.depvar[:u1] or res.u.depvar[Symbol(:u,1)] as shown here.","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"Subsetting the array also works, but is inelegant.","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"(If param_estim == true, then res.u.p are the fit parameters)","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"If Flux.jl is used, then subsetting the array is required. This looks like:","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"init_params = [Flux.destructure(c)[1] for c in chain]\nacum = [0; accumulate(+, length.(init_params))]\nsep = [(acum[i] + 1):acum[i + 1] for i in 1:(length(acum) - 1)]\nminimizers_ = [res.minimizer[s] for s in sep]","category":"page"},{"location":"tutorials/systems/#Note:-Solving-Matrices-of-PDEs","page":"Defining Systems of PDEs","title":"Note: Solving Matrices of PDEs","text":"","category":"section"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"Also, in addition to vector systems, we can use the matrix form of PDEs:","category":"page"},{"location":"tutorials/systems/","page":"Defining Systems of PDEs","title":"Defining Systems of PDEs","text":"using ModelingToolkit, NeuralPDE\n@parameters x y\n@variables u[1:2, 1:2](..)\n@derivatives Dxx'' ~ x\n@derivatives Dyy'' ~ y\n\n# Initial and boundary conditions\nbcs = [u[1](x, 0) ~ x, u[2](x, 0) ~ 2, u[3](x, 0) ~ 3, u[4](x, 0) ~ 4]\n\n# matrix PDE\neqs = @. [(Dxx(u_(x, y)) + Dyy(u_(x, y))) for u_ in u] ~ -sin(pi * x) * sin(pi * y) *\n                                                         [0 1; 0 1]\n\nsize(eqs)","category":"page"},{"location":"manual/logging/#Logging-Utilities","page":"Logging Utilities","title":"Logging Utilities","text":"","category":"section"},{"location":"manual/logging/","page":"Logging Utilities","title":"Logging Utilities","text":"LogOptions","category":"page"},{"location":"manual/logging/#NeuralPDE.LogOptions","page":"Logging Utilities","title":"NeuralPDE.LogOptions","text":"???\n\n\n\n\n\n","category":"type"},{"location":"manual/pinns/#PhysicsInformedNN-Discretizer-for-PDESystems","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"","category":"section"},{"location":"manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"Using the PINNs solver, we can solve general nonlinear PDEs:","category":"page"},{"location":"manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"(Image: generalPDE)","category":"page"},{"location":"manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"with suitable boundary conditions:","category":"page"},{"location":"manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"(Image: bcs)","category":"page"},{"location":"manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"where time t is a special component of x, and Ω contains the temporal domain.","category":"page"},{"location":"manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"PDEs are defined using the ModelingToolkit.jl PDESystem:","category":"page"},{"location":"manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"@named pde_system = PDESystem(eq, bcs, domains, param, var)","category":"page"},{"location":"manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"Here, eq is the equation, bcs represents the boundary conditions, param is the parameter of the equation (like [x,y]), and var represents variables (like [u]). For more information, see the ModelingToolkit.jl PDESystem documentation.","category":"page"},{"location":"manual/pinns/#The-PhysicsInformedNN-Discretizer","page":"PhysicsInformedNN Discretizer for PDESystems","title":"The PhysicsInformedNN Discretizer","text":"","category":"section"},{"location":"manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"NeuralPDE.PhysicsInformedNN\nNeuralPDE.Phi\nSciMLBase.discretize(::PDESystem, ::NeuralPDE.PhysicsInformedNN)","category":"page"},{"location":"manual/pinns/#NeuralPDE.PhysicsInformedNN","page":"PhysicsInformedNN Discretizer for PDESystems","title":"NeuralPDE.PhysicsInformedNN","text":"PhysicsInformedNN(chain,\n                  strategy;\n                  init_params = nothing,\n                  phi = nothing,\n                  param_estim = false,\n                  additional_loss = nothing,\n                  adaptive_loss = nothing,\n                  logger = nothing,\n                  log_options = LogOptions(),\n                  iteration = nothing,\n                  kwargs...) where {iip}\n\nA discretize algorithm for the ModelingToolkit PDESystem interface, which transforms a PDESystem into an OptimizationProblem using the Physics-Informed Neural Networks (PINN) methodology.\n\nPositional Arguments\n\nchain: a vector of Flux.jl or Lux.jl chains with a d-dimensional input and a 1-dimensional output corresponding to each of the dependent variables. Note that this specification respects the order of the dependent variables as specified in the PDESystem.\nstrategy: determines which training strategy will be used. See the Training Strategy documentation for more details.\n\nKeyword Arguments\n\ninit_params: the initial parameters of the neural networks. This should match the specification of the chosen chain library. For example, if a Flux.chain is used, then init_params should match Flux.destructure(chain)[1] in shape. If init_params is not given, then the neural network default parameters are used. Note that for Lux, the default will convert to Float64.\nphi: a trial solution, specified as phi(x,p) where x is the coordinates vector for the dependent variable and p are the weights of the phi function (generally the weights of the neural network defining phi). By default, this is generated from the chain. This should only be used to more directly impose functional information in the training problem, for example imposing the boundary condition by the test function formulation.\nadaptive_loss: the choice for the adaptive loss function. See the adaptive loss page for more details. Defaults to no adaptivity.\nadditional_loss: a function additional_loss(phi, θ, p_) where phi are the neural network trial solutions, θ are the weights of the neural network(s), and p_ are the hyperparameters of the OptimizationProblem. If param_estim = true, then θ additionally contains the parameters of the differential equation appended to the end of the vector.\nparam_estim: whether the parameters of the differential equation should be included in the values sent to the additional_loss function. Defaults to false.\nlogger: ?? needs docs\nlog_options: ?? why is this separate from the logger?\niteration: used to control the iteration counter???\nkwargs: Extra keyword arguments which are splatted to the OptimizationProblem on solve.\n\n\n\n\n\n","category":"type"},{"location":"manual/pinns/#NeuralPDE.Phi","page":"PhysicsInformedNN Discretizer for PDESystems","title":"NeuralPDE.Phi","text":"An encoding of the test function phi that is used for calculating the PDE value at domain points x\n\nFields:\n\nf: A representation of the chain function. If FastChain, then f(x,p), if Chain then f(p)(x) (from Flux.destructure)\nst: The state of the Lux.AbstractExplicitLayer. If a Flux.Chain then this is nothing. It should be updated on each call.\n\n\n\n\n\n","category":"type"},{"location":"manual/pinns/#SciMLBase.discretize-Tuple{PDESystem, PhysicsInformedNN}","page":"PhysicsInformedNN Discretizer for PDESystems","title":"SciMLBase.discretize","text":"prob = discretize(pde_system::PDESystem, discretization::PhysicsInformedNN)\n\nTransforms a symbolic description of a ModelingToolkit-defined PDESystem and generates an OptimizationProblem for Optimization.jl whose solution is the solution to the PDE.\n\n\n\n\n\n","category":"method"},{"location":"manual/pinns/#symbolic_discretize-and-the-lower-level-interface","page":"PhysicsInformedNN Discretizer for PDESystems","title":"symbolic_discretize and the lower-level interface","text":"","category":"section"},{"location":"manual/pinns/","page":"PhysicsInformedNN Discretizer for PDESystems","title":"PhysicsInformedNN Discretizer for PDESystems","text":"SciMLBase.symbolic_discretize(::PDESystem, ::NeuralPDE.PhysicsInformedNN)\nNeuralPDE.PINNRepresentation\nNeuralPDE.PINNLossFunctions","category":"page"},{"location":"manual/pinns/#SciMLBase.symbolic_discretize-Tuple{PDESystem, PhysicsInformedNN}","page":"PhysicsInformedNN Discretizer for PDESystems","title":"SciMLBase.symbolic_discretize","text":"prob = symbolic_discretize(pde_system::PDESystem, discretization::PhysicsInformedNN)\n\nsymbolic_discretize is the lower level interface to discretize for inspecting internals. It transforms a symbolic description of a ModelingToolkit-defined PDESystem into a PINNRepresentation which holds the pieces required to build an OptimizationProblem for Optimization.jl whose solution is the solution to the PDE.\n\nFor more information, see discretize and PINNRepresentation.\n\n\n\n\n\n","category":"method"},{"location":"manual/pinns/#NeuralPDE.PINNRepresentation","page":"PhysicsInformedNN Discretizer for PDESystems","title":"NeuralPDE.PINNRepresentation","text":"PINNRepresentation`\n\nAn internal representation of a physics-informed neural network (PINN). This is the struct used internally and returned for introspection by symbolic_discretize.\n\nFields\n\neqs: The equations of the PDE\n\nbcs: The boundary condition equations\n\ndomains: The domains for each of the independent variables\n\neq_params: ???\n\ndefaults: ???\n\ndefault_p: ???\n\nparam_estim: Whether parameters are to be appended to the additional_loss\n\nadditional_loss: The additional_loss function as provided by the user\n\nadaloss: The adaptive loss function\n\ndepvars: The dependent variables of the system\n\nindvars: The independent variables of the system\n\ndict_indvars: A dictionary form of the independent variables. Define the structure ???\n\ndict_depvars: A dictionary form of the dependent variables. Define the structure ???\n\ndict_depvar_input: ???\n\nlogger: The logger as provided by the user\n\nmultioutput: Whether there are multiple outputs, i.e. a system of PDEs\n\niteration: The iteration counter used inside the cost function\n\ninit_params: The initial parameters as provided by the user. If the PDE is a system of PDEs, this will be an array of arrays. If Lux.jl is used, then this is an array of ComponentArrays.\n\nflat_init_params: The initial parameters as a flattened array. This is the array that is used in the construction of the OptimizationProblem. If a Lux.jl neural network is used, then this flattened form is a ComponentArray. If the equation is a system of equations, then flat_init_params.depvar.x are the parameters for the neural network corresponding to the dependent variable x, and i.e. if depvar[i] == :x then for phi[i]. If param_estim = true, then flat_init_params.p are the parameters and flat_init_params.depvar.x are the neural network parameters, so flat_init_params.depvar.x would be the parameters of the neural network for the dependent variable x if it's a system. If a Flux.jl neural network is used, this is simply an AbstractArray to be indexed and the sizes from the chains must be remembered/stored/used.\n\nphi: The representation of the test function of the PDE solution\n\nderivative: The function used for computing the derivative\n\nstrategy: The training strategy as provided by the user\n\npde_indvars: ???\n\nbc_indvars: ???\n\npde_integration_vars: ???\n\nbc_integration_vars: ???\n\nintegral: ???\n\nsymbolic_pde_loss_functions: The PDE loss functions as represented in Julia AST\n\nsymbolic_bc_loss_functions: The boundary condition loss functions as represented in Julia AST\n\nloss_functions: The PINNLossFunctions, i.e. the generated loss functions\n\n\n\n\n\n","category":"type"},{"location":"manual/pinns/#NeuralPDE.PINNLossFunctions","page":"PhysicsInformedNN Discretizer for PDESystems","title":"NeuralPDE.PINNLossFunctions","text":"PINNLossFunctions`\n\nThe generated functions from the PINNRepresentation\n\nFields\n\nbc_loss_functions: The boundary condition loss functions\n\npde_loss_functions: The PDE loss functions\n\nfull_loss_function: The full loss function, combining the PDE and boundary condition loss functions. This is the loss function that is used by the optimizer.\n\nadditional_loss_function: The wrapped additional_loss, as pieced together for the optimizer.\n\ndatafree_pde_loss_functions: The pre-data version of the PDE loss function\n\ndatafree_bc_loss_functions: The pre-data version of the BC loss function\n\n\n\n\n\n","category":"type"},{"location":"tutorials/neural_adapter/#Transfer-Learning-with-Neural-Adapter","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"","category":"section"},{"location":"tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"warning: Warning\nThis documentation page is out of date.","category":"page"},{"location":"tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"Transfer learning is a machine learning technique where a model trained on one task is re-purposed on a second related task.","category":"page"},{"location":"tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"neural_adapter is the method that trains a neural network using the results from an already obtained prediction.","category":"page"},{"location":"tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"This allows reusing the obtained prediction results and pre-training states of the neural network to get a new prediction, or reuse the results of predictions to train a related task (for example, the same task with a different domain). It makes it possible to create more flexible training schemes.","category":"page"},{"location":"tutorials/neural_adapter/#Retrain-the-prediction","page":"Transfer Learning with Neural Adapter","title":"Retrain the prediction","text":"","category":"section"},{"location":"tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"Using the example of 2D Poisson equation, it is shown how, using the method neural_adapter, to retrain the prediction of one neural network to another.","category":"page"},{"location":"tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"(Image: image)","category":"page"},{"location":"tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL, DiffEqBase\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters x y\n@variables u(..)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\n\n# 2D PDE\neq = Dxx(u(x, y)) + Dyy(u(x, y)) ~ -sin(pi * x) * sin(pi * y)\n\n# Initial and boundary conditions\nbcs = [u(0, y) ~ 0.0, u(1, y) ~ -sin(pi * 1) * sin(pi * y),\n    u(x, 0) ~ 0.0, u(x, 1) ~ -sin(pi * x) * sin(pi * 1)]\n# Space and time domains\ndomains = [x ∈ Interval(0.0, 1.0),\n    y ∈ Interval(0.0, 1.0)]\nquadrature_strategy = NeuralPDE.QuadratureTraining(reltol = 1e-2, abstol = 1e-2,\n                                                   maxiters = 50, batch = 100)\ninner = 8\naf = Lux.tanh\nchain1 = Chain(Dense(2, inner, af),\n               Dense(inner, inner, af),\n               Dense(inner, 1))\n\ndiscretization = NeuralPDE.PhysicsInformedNN(chain1,\n                                             quadrature_strategy)\n\n@named pde_system = PDESystem(eq, bcs, domains, [x, y], [u(x, y)])\nprob = NeuralPDE.discretize(pde_system, discretization)\nsym_prob = NeuralPDE.symbolic_discretize(pde_system, discretization)\n\nres = Optimization.solve(prob, BFGS(); maxiters = 2000)\nphi = discretization.phi\n\ninner_ = 12\naf = Lux.tanh\nchain2 = Lux.Chain(Dense(2, inner_, af),\n                   Dense(inner_, inner_, af),\n                   Dense(inner_, inner_, af),\n                   Dense(inner_, 1))\n\ninit_params2 = Float64.(ComponentArray(Lux.setup(Random.default_rng(), chain)[1]))\n\n# the rule by which the training will take place is described here in loss function\nfunction loss(cord, θ)\n    chain2(cord, θ) .- phi(cord, res.u)\nend\n\nstrategy = NeuralPDE.GridTraining(0.02)\n\nprob_ = NeuralPDE.neural_adapter(loss, init_params2, pde_system, strategy)\ncallback = function (p, l)\n    println(\"Current loss is: $l\")\n    return false\nend\nres_ = Optimization.solve(prob_, BFGS(); callback = callback, maxiters = 1000)\n\nphi_ = NeuralPDE.get_phi(chain2)\n\nxs, ys = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\nanalytic_sol_func(x, y) = (sin(pi * x) * sin(pi * y)) / (2pi^2)\n\nu_predict = reshape([first(phi([x, y], res.u)) for x in xs for y in ys],\n                    (length(xs), length(ys)))\nu_predict_ = reshape([first(phi_([x, y], res_.minimizer)) for x in xs for y in ys],\n                     (length(xs), length(ys)))\nu_real = reshape([analytic_sol_func(x, y) for x in xs for y in ys],\n                 (length(xs), length(ys)))\ndiff_u = u_predict .- u_real\ndiff_u_ = u_predict_ .- u_real\n\nusing Plots\np1 = plot(xs, ys, u_predict, linetype = :contourf, title = \"first predict\");\np2 = plot(xs, ys, u_predict_, linetype = :contourf, title = \"second predict\");\np3 = plot(xs, ys, u_real, linetype = :contourf, title = \"analytic\");\np4 = plot(xs, ys, diff_u, linetype = :contourf, title = \"error 1\");\np5 = plot(xs, ys, diff_u_, linetype = :contourf, title = \"error 2\");\nplot(p1, p2, p3, p4, p5)","category":"page"},{"location":"tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"(Image: neural_adapter)","category":"page"},{"location":"tutorials/neural_adapter/#Domain-decomposition","page":"Transfer Learning with Neural Adapter","title":"Domain decomposition","text":"","category":"section"},{"location":"tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"In this example, we first obtain a prediction of 2D Poisson equation on subdomains. We split up full domain into 10 sub problems by x, and create separate neural networks for each sub interval. If x domain ∈ [x0, xend] so, it is decomposed on 10 part: sub x domains = {[x0, x1], ... [xi,xi+1], ..., x9,xend]}. And then using the method neural_adapter, we retrain the banch of 10 predictions to the one prediction for full domain of task.","category":"page"},{"location":"tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"(Image: domain_decomposition)","category":"page"},{"location":"tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL, DiffEqBase\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters x y\n@variables u(..)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\n\neq = Dxx(u(x, y)) + Dyy(u(x, y)) ~ -sin(pi * x) * sin(pi * y)\n\nbcs = [u(0, y) ~ 0.0, u(1, y) ~ -sin(pi * 1) * sin(pi * y),\n    u(x, 0) ~ 0.0, u(x, 1) ~ -sin(pi * x) * sin(pi * 1)]\n\n# Space\nx_0 = 0.0\nx_end = 1.0\nx_domain = Interval(x_0, x_end)\ny_domain = Interval(0.0, 1.0)\ndomains = [x ∈ x_domain,\n    y ∈ y_domain]\n\ncount_decomp = 10\n\n# Neural network\naf = Lux.tanh\ninner = 10\nchains = [Lux.Chain(Dense(2, inner, af), Dense(inner, inner, af), Dense(inner, 1))\n          for _ in 1:count_decomp]\ninit_params = map(c -> Float64.(ComponentArray(Lux.setup(Random.default_rng(), c)[1])),\n                  chains)\n\nxs_ = infimum(x_domain):(1 / count_decomp):supremum(x_domain)\nxs_domain = [(xs_[i], xs_[i + 1]) for i in 1:(length(xs_) - 1)]\ndomains_map = map(xs_domain) do (xs_dom)\n    x_domain_ = Interval(xs_dom...)\n    domains_ = [x ∈ x_domain_,\n        y ∈ y_domain]\nend\n\nanalytic_sol_func(x, y) = (sin(pi * x) * sin(pi * y)) / (2pi^2)\nfunction create_bcs(x_domain_, phi_bound)\n    x_0, x_e = x_domain_.left, x_domain_.right\n    if x_0 == 0.0\n        bcs = [u(0, y) ~ 0.0,\n            u(x_e, y) ~ analytic_sol_func(x_e, y),\n            u(x, 0) ~ 0.0,\n            u(x, 1) ~ -sin(pi * x) * sin(pi * 1)]\n        return bcs\n    end\n    bcs = [u(x_0, y) ~ phi_bound(x_0, y),\n        u(x_e, y) ~ analytic_sol_func(x_e, y),\n        u(x, 0) ~ 0.0,\n        u(x, 1) ~ -sin(pi * x) * sin(pi * 1)]\n    bcs\nend\n\nreses = []\nphis = []\npde_system_map = []\n\nfor i in 1:count_decomp\n    println(\"decomposition $i\")\n    domains_ = domains_map[i]\n    phi_in(cord) = phis[i - 1](cord, reses[i - 1].minimizer)\n    phi_bound(x, y) = phi_in(vcat(x, y))\n    @register phi_bound(x, y)\n    Base.Broadcast.broadcasted(::typeof(phi_bound), x, y) = phi_bound(x, y)\n    bcs_ = create_bcs(domains_[1].domain, phi_bound)\n    @named pde_system_ = PDESystem(eq, bcs_, domains_, [x, y], [u(x, y)])\n    push!(pde_system_map, pde_system_)\n    strategy = NeuralPDE.GridTraining([0.1 / count_decomp, 0.1])\n\n    discretization = NeuralPDE.PhysicsInformedNN(chains[i], strategy;\n                                                 init_params = init_params[i])\n\n    prob = NeuralPDE.discretize(pde_system_, discretization)\n    symprob = NeuralPDE.symbolic_discretize(pde_system_, discretization)\n    res_ = Optimization.solve(prob, BFGS(), maxiters = 1000)\n    phi = discretization.phi\n    push!(reses, res_)\n    push!(phis, phi)\nend\n\nfunction compose_result(dx)\n    u_predict_array = Float64[]\n    diff_u_array = Float64[]\n    ys = infimum(domains[2].domain):dx:supremum(domains[2].domain)\n    xs_ = infimum(x_domain):dx:supremum(x_domain)\n    xs = collect(xs_)\n    function index_of_interval(x_)\n        for (i, x_domain) in enumerate(xs_domain)\n            if x_ <= x_domain[2] && x_ >= x_domain[1]\n                return i\n            end\n        end\n    end\n    for x_ in xs\n        i = index_of_interval(x_)\n        u_predict_sub = [first(phis[i]([x_, y], reses[i].minimizer)) for y in ys]\n        u_real_sub = [analytic_sol_func(x_, y) for y in ys]\n        diff_u_sub = abs.(u_predict_sub .- u_real_sub)\n        append!(u_predict_array, u_predict_sub)\n        append!(diff_u_array, diff_u_sub)\n    end\n    xs, ys = [infimum(d.domain):dx:supremum(d.domain) for d in domains]\n    u_predict = reshape(u_predict_array, (length(xs), length(ys)))\n    diff_u = reshape(diff_u_array, (length(xs), length(ys)))\n    u_predict, diff_u\nend\ndx = 0.01\nu_predict, diff_u = compose_result(dx)\n\ninner_ = 18\naf = Lux.tanh\nchain2 = Lux.Chain(Dense(2, inner_, af),\n                   Dense(inner_, inner_, af),\n                   Dense(inner_, inner_, af),\n                   Dense(inner_, inner_, af),\n                   Dense(inner_, 1))\n\ninit_params2 = Float64.(ComponentArray(Lux.setup(Random.default_rng(), chain2)[1]))\n\n@named pde_system = PDESystem(eq, bcs, domains, [x, y], [u(x, y)])\n\nlosses = map(1:count_decomp) do i\n    loss(cord, θ) = chain2(cord, θ) .- phis[i](cord, reses[i].minimizer)\nend\n\ncallback = function (p, l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nprob_ = NeuralPDE.neural_adapter(losses, init_params2, pde_system_map,\n                                 NeuralPDE.GridTraining([0.1 / count_decomp, 0.1]))\nres_ = Optimization.solve(prob_, BFGS(); callback = callback, maxiters = 2000)\nprob_ = NeuralPDE.neural_adapter(losses, res_.minimizer, pde_system_map,\n                                 NeuralPDE.GridTraining([0.05 / count_decomp, 0.05]))\nres_ = Optimization.solve(prob_, BFGS(); callback = callback, maxiters = 1000)\n\nphi_ = NeuralPDE.get_phi(chain2)\n\nxs, ys = [infimum(d.domain):dx:supremum(d.domain) for d in domains]\nu_predict_ = reshape([first(phi_([x, y], res_.minimizer)) for x in xs for y in ys],\n                     (length(xs), length(ys)))\nu_real = reshape([analytic_sol_func(x, y) for x in xs for y in ys],\n                 (length(xs), length(ys)))\ndiff_u_ = u_predict_ .- u_real\n\nusing Plots\n\np1 = plot(xs, ys, u_predict, linetype = :contourf, title = \"predict 1\");\np2 = plot(xs, ys, u_predict_, linetype = :contourf, title = \"predict 2\");\np3 = plot(xs, ys, u_real, linetype = :contourf, title = \"analytic\");\np4 = plot(xs, ys, diff_u, linetype = :contourf, title = \"error 1\");\np5 = plot(xs, ys, diff_u_, linetype = :contourf, title = \"error 2\");\nplot(p1, p2, p3, p4, p5)","category":"page"},{"location":"tutorials/neural_adapter/","page":"Transfer Learning with Neural Adapter","title":"Transfer Learning with Neural Adapter","text":"(Image: decomp)","category":"page"},{"location":"tutorials/ode/#Solving-ODEs-with-Physics-Informed-Neural-Networks-(PINNs)","page":"Introduction to NeuralPDE for ODEs","title":"Solving ODEs with Physics-Informed Neural Networks (PINNs)","text":"","category":"section"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"note: Note\nIt is highly recommended you first read the solving ordinary differential equations with DifferentialEquations.jl tutorial before reading this tutorial.","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"This tutorial is an introduction to using physics-informed neural networks (PINNs) for solving ordinary differential equations (ODEs). In contrast to the later parts of this documentation which use the symbolic interface, here we will focus on the simplified NNODE which uses the ODEProblem specification for the ODE. Mathematically, the ODEProblem defines a problem:","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"u = f(upt)","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"for t in (t_0t_f) with an initial condition u(t_0) = u_0. With physics-informed neural networks, we choose a neural network architecture NN to represent the solution u and seek parameters p such that NN' = f(NN,p,t) for all points in the domain. When this is satisfied sufficiently closely, then NN is thus a solution to the differential equation.","category":"page"},{"location":"tutorials/ode/#Solving-an-ODE-with-NNODE","page":"Introduction to NeuralPDE for ODEs","title":"Solving an ODE with NNODE","text":"","category":"section"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"Let's solve the simple ODE:","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"u = cos(2pi t)","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"for t in (01) and u_0 = 0 with NNODE. First, we define the ODEProblem as we would with any other DifferentialEquations.jl solver. This looks like:","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"using NeuralPDE, Flux, OptimizationOptimisers\n\nlinear(u, p, t) = cos(2pi * t)\ntspan = (0.0f0, 1.0f0)\nu0 = 0.0f0\nprob = ODEProblem(linear, u0, tspan)","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"Now, to define the NNODE solver, we must choose a neural network architecture. To do this, we will use the Flux.jl library to define a multilayer perceptron (MLP) with one hidden layer of 5 nodes and a sigmoid activation function. This looks like:","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"chain = Flux.Chain(Dense(1, 5, σ), Dense(5, 1))","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"Now we must choose an optimizer to define the NNODE solver. A common choice is ADAM, with a tunable rate , which we will set to 0.1. In general, this rate parameter should be decreased if the solver's loss tends to be unsteady (sometimes rise “too much”), but should be as large as possible for efficiency. Thus, the definition of the NNODE solver is as follows:","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"opt = OptimizationOptimisers.Adam(0.1)\nalg = NeuralPDE.NNODE(chain, opt)","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"Once these pieces are together, we call solve just like with any other ODEProblem solver. Let's turn on verbose so we can see the loss over time during the training process:","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"sol = solve(prob, NeuralPDE.NNODE(chain, opt), verbose = true, abstol = 1.0f-6,\n            maxiters = 200)","category":"page"},{"location":"tutorials/ode/","page":"Introduction to NeuralPDE for ODEs","title":"Introduction to NeuralPDE for ODEs","text":"And that's it: the neural network solution was computed by training the neural network and returned in the standard DifferentialEquations.jl ODESolution format. For more information on handling the solution, consult the DifferentialEquations.jl solution handling section.","category":"page"},{"location":"tutorials/integro_diff/#Solving-Integro-Differential-Equations-with-Physics-Informed-Neural-Networks-(PINNs)","page":"Solving Integro Differential Equations","title":"Solving Integro-Differential Equations with Physics-Informed Neural Networks (PINNs)","text":"","category":"section"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"The integral of function u(x),","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"int_0^tu(x)dx","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"where x is variable of integral and t is variable of integro-differential equation,","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"is defined as","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"using ModelingToolkit\n@parameters t\n@variables i(..)\nIi = Symbolics.Integral(t in DomainSets.ClosedInterval(0, t))","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"In multidimensional case,","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"Ix = Integral((x, y) in DomainSets.UnitSquare())","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"The UnitSquare domain ranges both x and y from 0 to 1. Similarly, a rectangular or cuboidal domain can be defined using ProductDomain of ClosedIntervals.","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"Ix = Integral((x, y) in DomainSets.ProductDomain(ClosedInterval(0, 1), ClosedInterval(0, x)))","category":"page"},{"location":"tutorials/integro_diff/#dimensional-example","page":"Solving Integro Differential Equations","title":"1-dimensional example","text":"","category":"section"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"Let's take an example of an integro-differential equation:","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"fract u(t)  + 2u(t) + 5 int_0^tu(x)dx = 1  textfor  t geq 0","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"and boundary condition","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"u(0) = 0","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"using NeuralPDE, Flux, ModelingToolkit, Optimization, OptimizationOptimJL, DomainSets\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t\n@variables i(..)\nDi = Differential(t)\nIi = Integral(t in DomainSets.ClosedInterval(0, t))\neq = Di(i(t)) + 2 * i(t) + 5 * Ii(i(t)) ~ 1\nbcs = [i(0.0) ~ 0.0]\ndomains = [t ∈ Interval(0.0, 2.0)]\nchain = Chain(Dense(1, 15, Flux.σ), Dense(15, 1)) |> f64\n\nstrategy_ = GridTraining(0.05)\ndiscretization = PhysicsInformedNN(chain,\n                                   strategy_)\n@named pde_system = PDESystem(eq, bcs, domains, [t], [i(t)])\nprob = NeuralPDE.discretize(pde_system, discretization)\ncallback = function (p, l)\n    println(\"Current loss is: $l\")\n    return false\nend\nres = Optimization.solve(prob, BFGS(); callback = callback, maxiters = 100)","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"Plotting the final solution and analytical solution","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"ts = [infimum(d.domain):0.01:supremum(d.domain) for d in domains][1]\nphi = discretization.phi\nu_predict = [first(phi([t], res.u)) for t in ts]\n\nanalytic_sol_func(t) = 1 / 2 * (exp(-t)) * (sin(2 * t))\nu_real = [analytic_sol_func(t) for t in ts]\nusing Plots\nplot(ts, u_real, label = \"Analytical Solution\")\nplot!(ts, u_predict, label = \"PINN Solution\")","category":"page"},{"location":"tutorials/integro_diff/","page":"Solving Integro Differential Equations","title":"Solving Integro Differential Equations","text":"(Image: IDE)","category":"page"},{"location":"examples/3rd/#ODE-with-a-3rd-Order-Derivative","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"","category":"section"},{"location":"examples/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"Let's consider the ODE with a 3rd-order derivative:","category":"page"},{"location":"examples/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"beginalign*\n^3_x u(x) = cos(pi x)  \nu(0) = 0  \nu(1) = cos(pi)  \n_x u(0) = 1  \nx in 0 1  \nendalign*","category":"page"},{"location":"examples/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"We will use physics-informed neural networks.","category":"page"},{"location":"examples/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"using NeuralPDE, Lux, ModelingToolkit\nusing Optimization, OptimizationOptimJL, OptimizationOptimisers\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters x\n@variables u(..)\n\nDxxx = Differential(x)^3\nDx = Differential(x)\n# ODE\neq = Dxxx(u(x)) ~ cos(pi * x)\n\n# Initial and boundary conditions\nbcs = [u(0.0) ~ 0.0,\n    u(1.0) ~ cos(pi),\n    Dx(u(1.0)) ~ 1.0]\n\n# Space and time domains\ndomains = [x ∈ Interval(0.0, 1.0)]\n\n# Neural network\nchain = Lux.Chain(Dense(1, 8, Lux.σ), Dense(8, 1))\n\ndiscretization = PhysicsInformedNN(chain, QuasiRandomTraining(20))\n@named pde_system = PDESystem(eq, bcs, domains, [x], [u(x)])\nprob = discretize(pde_system, discretization)\n\ncallback = function (p, l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nres = Optimization.solve(prob, Adam(0.01); callback = callback, maxiters = 2000)\nphi = discretization.phi","category":"page"},{"location":"examples/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"We can plot the predicted solution of the ODE and its analytical solution.","category":"page"},{"location":"examples/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"using Plots\n\nanalytic_sol_func(x) = (π * x * (-x + (π^2) * (2 * x - 3) + 1) - sin(π * x)) / (π^3)\n\ndx = 0.05\nxs = [infimum(d.domain):(dx / 10):supremum(d.domain) for d in domains][1]\nu_real = [analytic_sol_func(x) for x in xs]\nu_predict = [first(phi(x, res.u)) for x in xs]\n\nx_plot = collect(xs)\nplot(x_plot, u_real, title = \"real\")\nplot!(x_plot, u_predict, title = \"predict\")","category":"page"},{"location":"examples/3rd/","page":"ODE with a 3rd-Order Derivative","title":"ODE with a 3rd-Order Derivative","text":"(Image: hodeplot)","category":"page"},{"location":"tutorials/param_estim/#Optimizing-Parameters-(Solving-Inverse-Problems)-with-Physics-Informed-Neural-Networks-(PINNs)","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimizing Parameters (Solving Inverse Problems) with Physics-Informed Neural Networks (PINNs)","text":"","category":"section"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"Consider a Lorenz System,","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"beginalign*\n    fracmathrmd xmathrmdt = sigma (y -x)  \n    fracmathrmd ymathrmdt = x (rho - z) - y  \n    fracmathrmd zmathrmdt = x y - beta z  \nendalign*","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"with Physics-Informed Neural Networks. Now we would consider the case where we want to optimize the parameters \\sigma, \\beta, and \\rho.","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"We start by defining the problem,","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL, OrdinaryDiffEq,\n      Plots\nimport ModelingToolkit: Interval, infimum, supremum\n@parameters t, σ_, β, ρ\n@variables x(..), y(..), z(..)\nDt = Differential(t)\neqs = [Dt(x(t)) ~ σ_ * (y(t) - x(t)),\n    Dt(y(t)) ~ x(t) * (ρ - z(t)) - y(t),\n    Dt(z(t)) ~ x(t) * y(t) - β * z(t)]\n\nbcs = [x(0) ~ 1.0, y(0) ~ 0.0, z(0) ~ 0.0]\ndomains = [t ∈ Interval(0.0, 1.0)]\ndt = 0.01","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"And the neural networks as,","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"input_ = length(domains)\nn = 8\nchain1 = Lux.Chain(Dense(input_, n, Lux.σ), Dense(n, n, Lux.σ), Dense(n, n, Lux.σ),\n                   Dense(n, 1))\nchain2 = Lux.Chain(Dense(input_, n, Lux.σ), Dense(n, n, Lux.σ), Dense(n, n, Lux.σ),\n                   Dense(n, 1))\nchain3 = Lux.Chain(Dense(input_, n, Lux.σ), Dense(n, n, Lux.σ), Dense(n, n, Lux.σ),\n                   Dense(n, 1))","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"We will add another loss term based on the data that we have to optimize the parameters.","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"Here we simply calculate the solution of the Lorenz system with OrdinaryDiffEq.jl based on the adaptivity of the ODE solver. This is used to introduce non-uniformity to the time series.","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"function lorenz!(du, u, p, t)\n    du[1] = 10.0 * (u[2] - u[1])\n    du[2] = u[1] * (28.0 - u[3]) - u[2]\n    du[3] = u[1] * u[2] - (8 / 3) * u[3]\nend\n\nu0 = [1.0; 0.0; 0.0]\ntspan = (0.0, 1.0)\nprob = ODEProblem(lorenz!, u0, tspan)\nsol = solve(prob, Tsit5(), dt = 0.1)\nts = [infimum(d.domain):dt:supremum(d.domain) for d in domains][1]\nfunction getData(sol)\n    data = []\n    us = hcat(sol(ts).u...)\n    ts_ = hcat(sol(ts).t...)\n    return [us, ts_]\nend\ndata = getData(sol)\n\n(u_, t_) = data\nlen = length(data[2])","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"Then we define the additional loss function additional_loss(phi, θ , p), the function has three arguments:","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"phi the trial solution\nθ the parameters of neural networks\nthe hyperparameters p .","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"For a Lux neural network, the composed function will present itself as having θ as a ComponentArray subsets θ.x, which can also be dereferenced like θ[:x]. Thus, the additional loss looks like:","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"depvars = [:x, :y, :z]\nfunction additional_loss(phi, θ, p)\n    return sum(sum(abs2, phi[i](t_, θ[depvars[i]]) .- u_[[i], :]) / len for i in 1:1:3)\nend","category":"page"},{"location":"tutorials/param_estim/#Note-about-Flux","page":"Optimising Parameters (Solving Inverse Problems)","title":"Note about Flux","text":"","category":"section"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"If Flux neural network is used, then the subsetting must be computed manually as θ is simply a vector. This looks like:","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"init_params = [Flux.destructure(c)[1] for c in [chain1, chain2, chain3]]\nacum = [0; accumulate(+, length.(init_params))]\nsep = [(acum[i] + 1):acum[i + 1] for i in 1:(length(acum) - 1)]\n(u_, t_) = data\nlen = length(data[2])\n\nfunction additional_loss(phi, θ, p)\n    return sum(sum(abs2, phi[i](t_, θ[sep[i]]) .- u_[[i], :]) / len for i in 1:1:3)\nend","category":"page"},{"location":"tutorials/param_estim/#Back-to-our-originally-scheduled-programming","page":"Optimising Parameters (Solving Inverse Problems)","title":"Back to our originally scheduled programming","text":"","category":"section"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"Then finally defining and optimizing using the PhysicsInformedNN interface.","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"discretization = NeuralPDE.PhysicsInformedNN([chain1, chain2, chain3],\n                                             NeuralPDE.GridTraining(dt), param_estim = true,\n                                             additional_loss = additional_loss)\n@named pde_system = PDESystem(eqs, bcs, domains, [t], [x(t), y(t), z(t)], [σ_, ρ, β],\n                              defaults = Dict([p .=> 1.0 for p in [σ_, ρ, β]]))\nprob = NeuralPDE.discretize(pde_system, discretization)\ncallback = function (p, l)\n    println(\"Current loss is: $l\")\n    return false\nend\nres = Optimization.solve(prob, BFGS(); callback = callback, maxiters = 5000)\np_ = res.u[(end - 2):end] # p_ = [9.93, 28.002, 2.667]","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"And then finally some analysis by plotting.","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"minimizers = [res.u.depvar[depvars[i]] for i in 1:3]\nts = [infimum(d.domain):(dt / 10):supremum(d.domain) for d in domains][1]\nu_predict = [[discretization.phi[i]([t], minimizers[i])[1] for t in ts] for i in 1:3]\nplot(sol)\nplot!(ts, u_predict, label = [\"x(t)\" \"y(t)\" \"z(t)\"])","category":"page"},{"location":"tutorials/param_estim/","page":"Optimising Parameters (Solving Inverse Problems)","title":"Optimising Parameters (Solving Inverse Problems)","text":"(Image: Plot_Lorenz)","category":"page"},{"location":"examples/nonlinear_elliptic/#Nonlinear-elliptic-system-of-PDEs","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"","category":"section"},{"location":"examples/nonlinear_elliptic/","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"We can also solve nonlinear systems, such as the system of nonlinear elliptic PDEs","category":"page"},{"location":"examples/nonlinear_elliptic/","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"beginaligned\nfracpartial^2upartial x^2 + fracpartial^2upartial y^2 = uf(fracuw) + fracuwh(fracuw) \nfracpartial^2wpartial x^2 + fracpartial^2wpartial y^2 = wg(fracuw) + h(fracuw) \nendaligned","category":"page"},{"location":"examples/nonlinear_elliptic/","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"where f, g, h are arbitrary functions. With initial and boundary conditions:","category":"page"},{"location":"examples/nonlinear_elliptic/","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"beginaligned\nu(0y) = y + 1 \nw(1 y) = cosh(sqrtf(k)) + sinh(sqrtf(k))cdot(y + 1) \nw(x0) = cosh(sqrtf(k)) + sinh(sqrtf(k)) \nw(0y) = k(y + 1) \nu(1 y) = kcosh(sqrtf(k)) + sinh(sqrtf(k))cdot(y + 1) \nu(x0) = kcosh(sqrtf(k)) + sinh(sqrtf(k)) \nendaligned","category":"page"},{"location":"examples/nonlinear_elliptic/","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"where k is a root of the algebraic (transcendental) equation f(k) = g(k).","category":"page"},{"location":"examples/nonlinear_elliptic/","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"This is done using a derivative neural network approximation.","category":"page"},{"location":"examples/nonlinear_elliptic/","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL, Roots\nusing Plots\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters x, y\nDx = Differential(x)\nDy = Differential(y)\n@variables Dxu(..), Dyu(..), Dxw(..), Dyw(..)\n@variables u(..), w(..)\n\n# Arbitrary functions\nf(x) = sin(x)\ng(x) = cos(x)\nh(x) = x\nroot(x) = f(x) - g(x)\n\n# Analytic solution\nk = find_zero(root, (0, 1), Bisection())                            # k is a root of the algebraic (transcendental) equation f(x) = g(x)\nθ(x, y) = (cosh(sqrt(f(k)) * x) + sinh(sqrt(f(k)) * x)) * (y + 1)   # Analytical solution to Helmholtz equation\nw_analytic(x, y) = θ(x, y) - h(k) / f(k)\nu_analytic(x, y) = k * w_analytic(x, y)\n\n# Nonlinear Steady-State Systems of Two Reaction-Diffusion Equations with 3 arbitrary function f, g, h\neqs_ = [\n    Dx(Dxu(x, y)) + Dy(Dyu(x, y)) ~ u(x, y) * f(u(x, y) / w(x, y)) +\n                                    u(x, y) / w(x, y) * h(u(x, y) / w(x, y)),\n    Dx(Dxw(x, y)) + Dy(Dyw(x, y)) ~ w(x, y) * g(u(x, y) / w(x, y)) + h(u(x, y) / w(x, y))]\n\n# Boundary conditions\nbcs_ = [u(0, y) ~ u_analytic(0, y),\n    u(1, y) ~ u_analytic(1, y),\n    u(x, 0) ~ u_analytic(x, 0),\n    w(0, y) ~ w_analytic(0, y),\n    w(1, y) ~ w_analytic(1, y),\n    w(x, 0) ~ w_analytic(x, 0)]\n\nder_ = [Dy(u(x, y)) ~ Dyu(x, y),\n    Dy(w(x, y)) ~ Dyw(x, y),\n    Dx(u(x, y)) ~ Dxu(x, y),\n    Dx(w(x, y)) ~ Dxw(x, y)]\n\nbcs__ = [bcs_; der_]\n\n# Space and time domains\ndomains = [x ∈ Interval(0.0, 1.0),\n    y ∈ Interval(0.0, 1.0)]\n\n# Neural network\ninput_ = length(domains)\nn = 15\nchain = [Lux.Chain(Dense(input_, n, Lux.σ), Dense(n, n, Lux.σ), Dense(n, 1)) for _ in 1:6] # 1:number of @variables\n\nstrategy = QuadratureTraining()\ndiscretization = PhysicsInformedNN(chain, strategy)\n\nvars = [u(x, y), w(x, y), Dxu(x, y), Dyu(x, y), Dxw(x, y), Dyw(x, y)]\n@named pdesystem = PDESystem(eqs_, bcs__, domains, [x, y], vars)\nprob = NeuralPDE.discretize(pdesystem, discretization)\nsym_prob = NeuralPDE.symbolic_discretize(pdesystem, discretization)\n\nstrategy = NeuralPDE.QuadratureTraining()\ndiscretization = PhysicsInformedNN(chain, strategy)\nsym_prob = NeuralPDE.symbolic_discretize(pdesystem, discretization)\n\npde_inner_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbcs_inner_loss_functions = sym_prob.loss_functions.bc_loss_functions[1:6]\naprox_derivative_loss_functions = sym_prob.loss_functions.bc_loss_functions[7:end]\n\ncallback = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p), bcs_inner_loss_functions))\n    println(\"der_losses: \", map(l_ -> l_(p), aprox_derivative_loss_functions))\n    return false\nend\n\nres = Optimization.solve(prob, BFGS(); callback = callback, maxiters = 5000)\n\nphi = discretization.phi\n\n# Analysis\nxs, ys = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\ndepvars = [:u, :w]\nminimizers_ = [res.u.depvar[depvars[i]] for i in 1:2]\n\nanalytic_sol_func(x, y) = [u_analytic(x, y), w_analytic(x, y)]\nu_real = [[analytic_sol_func(x, y)[i] for x in xs for y in ys] for i in 1:2]\nu_predict = [[phi[i]([x, y], minimizers_[i])[1] for x in xs for y in ys] for i in 1:2]\ndiff_u = [abs.(u_real[i] .- u_predict[i]) for i in 1:2]\nfor i in 1:2\n    p1 = plot(xs, ys, u_real[i], linetype = :contourf, title = \"u$i, analytic\")\n    p2 = plot(xs, ys, u_predict[i], linetype = :contourf, title = \"predict\")\n    p3 = plot(xs, ys, diff_u[i], linetype = :contourf, title = \"error\")\n    plot(p1, p2, p3)\n    savefig(\"non_linear_elliptic_sol_u$i\")\nend","category":"page"},{"location":"examples/nonlinear_elliptic/","page":"Nonlinear elliptic system of PDEs","title":"Nonlinear elliptic system of PDEs","text":"(Image: non_linear_elliptic_sol_u1) (Image: non_linear_elliptic_sol_u2)","category":"page"},{"location":"manual/neural_adapters/#Transfer-Learning-with-neural_adapter","page":"Transfer Learning with neural_adapter","title":"Transfer Learning with neural_adapter","text":"","category":"section"},{"location":"manual/neural_adapters/","page":"Transfer Learning with neural_adapter","title":"Transfer Learning with neural_adapter","text":"NeuralPDE.neural_adapter","category":"page"},{"location":"manual/neural_adapters/#NeuralPDE.neural_adapter","page":"Transfer Learning with neural_adapter","title":"NeuralPDE.neural_adapter","text":"neural_adapter(loss, init_params, pde_system, strategy)\n\nTrains a neural network using the results from one already obtained prediction.\n\nPositional Arguments\n\nloss: the body of loss function,\ninit_params: the initial parameter of the neural network,\npde_system: PDEs are defined using the ModelingToolkit.jl,\nstrategy: determines which training strategy will be used.\n\n\n\n\n\n","category":"function"},{"location":"examples/wave/#D-Wave-Equation-with-Dirichlet-boundary-conditions","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"","category":"section"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"Let's solve this 1-dimensional wave equation:","category":"page"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"beginalign*\n^2_t u(x t) = c^2 ^2_x u(x t) quad  textsffor all  0  x  1 text and  t  0   \nu(0 t) = u(1 t) = 0 quad  textsffor all  t  0   \nu(x 0) = x (1-x)     quad  textsffor all  0  x  1   \n_t u(x 0) = 0       quad  textsffor all  0  x  1   \nendalign*","category":"page"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"with grid discretization dx = 0.1 and physics-informed neural networks.","category":"page"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"Further, the solution of this equation with the given boundary conditions is presented.","category":"page"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"using NeuralPDE, Lux, Optimization, OptimizationOptimJL\nimport ModelingToolkit: Interval\n\n@parameters t, x\n@variables u(..)\nDxx = Differential(x)^2\nDtt = Differential(t)^2\nDt = Differential(t)\n\n#2D PDE\nC = 1\neq = Dtt(u(t, x)) ~ C^2 * Dxx(u(t, x))\n\n# Initial and boundary conditions\nbcs = [u(t, 0) ~ 0.0,# for all t > 0\n    u(t, 1) ~ 0.0,# for all t > 0\n    u(0, x) ~ x * (1.0 - x), #for all 0 < x < 1\n    Dt(u(0, x)) ~ 0.0] #for all  0 < x < 1]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0, 1.0),\n    x ∈ Interval(0.0, 1.0)]\n# Discretization\ndx = 0.1\n\n# Neural network\nchain = Lux.Chain(Dense(2, 16, Lux.σ), Dense(16, 16, Lux.σ), Dense(16, 1))\ndiscretization = PhysicsInformedNN(chain, GridTraining(dx))\n\n@named pde_system = PDESystem(eq, bcs, domains, [t, x], [u(t, x)])\nprob = discretize(pde_system, discretization)\n\ncallback = function (p, l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\n# optimizer\nopt = OptimizationOptimJL.BFGS()\nres = Optimization.solve(prob, opt; callback = callback, maxiters = 1200)\nphi = discretization.phi","category":"page"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"We can plot the predicted solution of the PDE and compare it with the analytical solution to plot the relative error.","category":"page"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"using Plots\n\nts, xs = [infimum(d.domain):dx:supremum(d.domain) for d in domains]\nfunction analytic_sol_func(t, x)\n    sum([(8 / (k^3 * pi^3)) * sin(k * pi * x) * cos(C * k * pi * t) for k in 1:2:50000])\nend\n\nu_predict = reshape([first(phi([t, x], res.u)) for t in ts for x in xs],\n                    (length(ts), length(xs)))\nu_real = reshape([analytic_sol_func(t, x) for t in ts for x in xs],\n                 (length(ts), length(xs)))\n\ndiff_u = abs.(u_predict .- u_real)\np1 = plot(ts, xs, u_real, linetype = :contourf, title = \"analytic\");\np2 = plot(ts, xs, u_predict, linetype = :contourf, title = \"predict\");\np3 = plot(ts, xs, diff_u, linetype = :contourf, title = \"error\");\nplot(p1, p2, p3)","category":"page"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"(Image: waveplot)","category":"page"},{"location":"examples/wave/#D-Damped-Wave-Equation-with-Dirichlet-boundary-conditions","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Damped Wave Equation with Dirichlet boundary conditions","text":"","category":"section"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"Now let's solve the 1-dimensional wave equation with damping.","category":"page"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"beginaligned\nfracpartial^2 u(tx)partial x^2 = frac1c^2 fracpartial^2 u(tx)partial t^2 + v fracpartial u(tx)partial t \nu(t 0) = u(t L) = 0 \nu(0 x) = x(1-x) \nu_t(0 x) = 1 - 2x \nendaligned","category":"page"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"with grid discretization dx = 0.05 and physics-informed neural networks. Here, we take advantage of adaptive derivative to increase accuracy.","category":"page"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL\nusing Plots, Printf\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\n@variables u(..) Dxu(..) Dtu(..) O1(..) O2(..)\nDxx = Differential(x)^2\nDtt = Differential(t)^2\nDx = Differential(x)\nDt = Differential(t)\n\n# Constants\nv = 3\nb = 2\nL = 1.0\n@assert b > 0 && b < 2π / (L * v)\n\n# 1D damped wave\neq = Dx(Dxu(t, x)) ~ 1 / v^2 * Dt(Dtu(t, x)) + b * Dtu(t, x)\n\n# Initial and boundary conditions\nbcs_ = [u(t, 0) ~ 0.0,# for all t > 0\n    u(t, L) ~ 0.0,# for all t > 0\n    u(0, x) ~ x * (1.0 - x), # for all 0 < x < 1\n    Dtu(0, x) ~ 1 - 2x, # for all  0 < x < 1\n]\n\nep = (cbrt(eps(eltype(Float64))))^2 / 6\n\nder = [Dxu(t, x) ~ Dx(u(t, x)) + ep * O1(t, x),\n    Dtu(t, x) ~ Dt(u(t, x)) + ep * O2(t, x)]\n\nbcs = [bcs_; der]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0, L),\n    x ∈ Interval(0.0, L)]\n\n# Neural network\ninn = 25\ninnd = 4\nchain = [[Lux.Chain(Dense(2, inn, Lux.tanh),\n                    Dense(inn, inn, Lux.tanh),\n                    Dense(inn, inn, Lux.tanh),\n                    Dense(inn, 1)) for _ in 1:3]\n         [Lux.Chain(Dense(2, innd, Lux.tanh), Dense(innd, 1)) for _ in 1:2]]\n\nstrategy = GridTraining(0.02)\ndiscretization = PhysicsInformedNN(chain, strategy;)\n\n@named pde_system = PDESystem(eq, bcs, domains, [t, x],\n                              [u(t, x), Dxu(t, x), Dtu(t, x), O1(t, x), O2(t, x)])\nprob = discretize(pde_system, discretization)\nsym_prob = NeuralPDE.symbolic_discretize(pde_system, discretization)\n\npde_inner_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbcs_inner_loss_functions = sym_prob.loss_functions.bc_loss_functions\n\ncallback = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p), bcs_inner_loss_functions))\n    return false\nend\n\nres = Optimization.solve(prob, BFGS(); callback = callback, maxiters = 2000)\nprob = remake(prob, u0 = res.u)\nres = Optimization.solve(prob, BFGS(); callback = callback, maxiters = 2000)\n\nphi = discretization.phi[1]\n\n# Analysis\nts, xs = [infimum(d.domain):0.05:supremum(d.domain) for d in domains]\n\nμ_n(k) = (v * sqrt(4 * k^2 * π^2 - b^2 * L^2 * v^2)) / (2 * L)\nfunction b_n(k)\n    2 / L * -(L^2 *\n      ((2 * π * L - π) * k * sin(π * k) + ((π^2 - π^2 * L) * k^2 + 2 * L) * cos(π * k) -\n       2 * L)) / (π^3 * k^3)\nend # vegas((x, ϕ) -> ϕ[1] = sin(k * π * x[1]) * f(x[1])).integral[1]\nfunction a_n(k)\n    2 / -(L * μ_n(k)) * (L * (((2 * π * L^2 - π * L) * b * k * sin(π * k) +\n       ((π^2 * L - π^2 * L^2) * b * k^2 + 2 * L^2 * b) * cos(π * k) - 2 * L^2 * b) * v^2 +\n      4 * π * L * k * sin(π * k) + (2 * π^2 - 4 * π^2 * L) * k^2 * cos(π * k) -\n      2 * π^2 * k^2)) / (2 * π^3 * k^3)\nend\n\n# Plot\nfunction analytic_sol_func(t, x)\n    sum([sin((k * π * x) / L) * exp(-v^2 * b * t / 2) *\n         (a_n(k) * sin(μ_n(k) * t) + b_n(k) * cos(μ_n(k) * t)) for k in 1:2:100])\nend # TODO replace 10 with 500\nanim = @animate for t in ts\n    @info \"Time $t...\"\n    sol = [analytic_sol_func(t, x) for x in xs]\n    sol_p = [first(phi([t, x], res.u.depvar.u)) for x in xs]\n    plot(sol, label = \"analytic\", ylims = [0, 0.1])\n    title = @sprintf(\"t = %.3f\", t)\n    plot!(sol_p, label = \"predict\", ylims = [0, 0.1], title = title)\nend\ngif(anim, \"1Dwave_damped_adaptive.gif\", fps = 200)\n\n# Surface plot\nts, xs = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\nu_predict = reshape([first(phi([t, x], res.u.depvar.u)) for\n                     t in ts for x in xs], (length(ts), length(xs)))\nu_real = reshape([analytic_sol_func(t, x) for t in ts for x in xs],\n                 (length(ts), length(xs)))\n\ndiff_u = abs.(u_predict .- u_real)\np1 = plot(ts, xs, u_real, linetype = :contourf, title = \"analytic\");\np2 = plot(ts, xs, u_predict, linetype = :contourf, title = \"predict\");\np3 = plot(ts, xs, diff_u, linetype = :contourf, title = \"error\");\nplot(p1, p2, p3)","category":"page"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"We can see the results here:","category":"page"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"(Image: Damped_wave_sol_adaptive_u)","category":"page"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"Plotted as a line, one can see the analytical solution and the prediction here:","category":"page"},{"location":"examples/wave/","page":"1D Wave Equation with Dirichlet boundary conditions","title":"1D Wave Equation with Dirichlet boundary conditions","text":"(Image: 1Dwave_damped_adaptive)","category":"page"},{"location":"manual/training_strategies/#Training-Strategies","page":"Training Strategies","title":"Training Strategies","text":"","category":"section"},{"location":"manual/training_strategies/","page":"Training Strategies","title":"Training Strategies","text":"Training strategies are the choices for how the points are sampled for the definition of the physics-informed loss.","category":"page"},{"location":"manual/training_strategies/#Recommendations","page":"Training Strategies","title":"Recommendations","text":"","category":"section"},{"location":"manual/training_strategies/","page":"Training Strategies","title":"Training Strategies","text":"QuasiRandomTraining with its default LatinHyperCubeSample() is a well-rounded training strategy which can be used for most situations. It scales well for high dimensional spaces and is GPU-compatible. QuadratureTraining can lead to faster or more robust convergence with one of the H-Cubature or P-Cubature methods, but are not currently GPU compatible. For very high dimensional cases, QuadratureTraining with an adaptive Monte Carlo quadrature method, such as CubaVegas, can be beneficial for difficult or stiff problems.","category":"page"},{"location":"manual/training_strategies/","page":"Training Strategies","title":"Training Strategies","text":"GridTraining should only be used for testing purposes and should not be relied upon for real training cases. StochasticTraining achieves a lower convergence rate in the quasi-Monte Carlo methods and thus QuasiRandomTraining should be preferred in most cases. WeightedIntervalTraining can only be used with ODEs (NNODE).","category":"page"},{"location":"manual/training_strategies/#API","page":"Training Strategies","title":"API","text":"","category":"section"},{"location":"manual/training_strategies/","page":"Training Strategies","title":"Training Strategies","text":"GridTraining\nStochasticTraining\nQuasiRandomTraining\nQuadratureTraining\nWeightedIntervalTraining","category":"page"},{"location":"manual/training_strategies/#NeuralPDE.GridTraining","page":"Training Strategies","title":"NeuralPDE.GridTraining","text":"GridTraining(dx)\n\nA training strategy that uses the grid points in a multidimensional grid with spacings dx. If the grid is multidimensional, then dx is expected to be an array of dx values matching the dimension of the domain, corresponding to the grid spacing in each dimension.\n\nPositional Arguments\n\ndx: the discretization of the grid.\n\n\n\n\n\n","category":"type"},{"location":"manual/training_strategies/#NeuralPDE.StochasticTraining","page":"Training Strategies","title":"NeuralPDE.StochasticTraining","text":"StochasticTraining(points; bcs_points = points)\n\nPositional Arguments\n\npoints: number of points in random select training set\n\nKeyword Arguments\n\nbcs_points: number of points in random select training set for boundary conditions (by default, it equals points).\n\n\n\n\n\n","category":"type"},{"location":"manual/training_strategies/#NeuralPDE.QuasiRandomTraining","page":"Training Strategies","title":"NeuralPDE.QuasiRandomTraining","text":"QuasiRandomTraining(points; bcs_points = points,\n                            sampling_alg = LatinHypercubeSample(), resampling = true,\n                            minibatch = 0)\n\nA training strategy which uses quasi-Monte Carlo sampling for low discrepancy sequences that accelerate the convergence in high dimensional spaces over pure random sequences.\n\nPositional Arguments\n\npoints:  the number of quasi-random points in a sample\n\nKeyword Arguments\n\nbcs_points: the number of quasi-random points in a sample for boundary conditions (by default, it equals points),\nsampling_alg: the quasi-Monte Carlo sampling algorithm,\nresampling: if it's false - the full training set is generated in advance before training,  and at each iteration, one subset is randomly selected out of the batch.  Ff it's true - the training set isn't generated beforehand, and one set of quasi-random  points is generated directly at each iteration in runtime. In this case, minibatch has no effect,\nminibatch: the number of subsets, if resampling == false.\n\nFor more information, see QuasiMonteCarlo.jl\n\n\n\n\n\n","category":"type"},{"location":"manual/training_strategies/#NeuralPDE.QuadratureTraining","page":"Training Strategies","title":"NeuralPDE.QuadratureTraining","text":"QuadratureTraining(; quadrature_alg = CubatureJLh(),\n                     reltol = 1e-6, abstol = 1e-3,\n                     maxiters = 1_000, batch = 100)\n\nA training strategy which treats the loss function as the integral of ||condition|| over the domain. Uses an Integrals.jl algorithm for computing the (adaptive) quadrature of this loss with respect to the chosen tolerances, with a batching batch corresponding to the maximum number of points to evaluate in a given integrand call.\n\nKeyword Arguments\n\nquadrature_alg: quadrature algorithm,\nreltol: relative tolerance,\nabstol: absolute tolerance,\nmaxiters: the maximum number of iterations in quadrature algorithm,\nbatch: the preferred number of points to batch.\n\nFor more information on the argument values and algorithm choices, see Integrals.jl.\n\n\n\n\n\n","category":"type"},{"location":"manual/training_strategies/#NeuralPDE.WeightedIntervalTraining","page":"Training Strategies","title":"NeuralPDE.WeightedIntervalTraining","text":"WeightedIntervalTraining(weights, samples)\n\nA training strategy that generates points for training based on the given inputs.  We split the timespan into equal segments based on the number of weights,  then sample points in each segment based on that segments corresponding weight, such that the total number of sampled points is equivalent to the given samples\n\nPositional Arguments\n\nweights: A vector of weights that should sum to 1, representing the proportion of samples at each interval.\nsamples: the total number of samples that we want, across the entire time span\n\nLimitations\n\nThis training strategy can only be used with ODEs (NNODE).\n\n\n\n\n\n","category":"type"},{"location":"examples/heterogeneous/#PDEs-with-Dependent-Variables-on-Heterogeneous-Domains","page":"PDEs with Dependent Variables on Heterogeneous Domains","title":"PDEs with Dependent Variables on Heterogeneous Domains","text":"","category":"section"},{"location":"examples/heterogeneous/","page":"PDEs with Dependent Variables on Heterogeneous Domains","title":"PDEs with Dependent Variables on Heterogeneous Domains","text":"A differential equation is said to have heterogeneous domains when its dependent variables depend on different independent variables:","category":"page"},{"location":"examples/heterogeneous/","page":"PDEs with Dependent Variables on Heterogeneous Domains","title":"PDEs with Dependent Variables on Heterogeneous Domains","text":"u(x) + w(x v) = fracpartial w(x v)partial w","category":"page"},{"location":"examples/heterogeneous/","page":"PDEs with Dependent Variables on Heterogeneous Domains","title":"PDEs with Dependent Variables on Heterogeneous Domains","text":"Here, we write an arbitrary heterogeneous system:","category":"page"},{"location":"examples/heterogeneous/","page":"PDEs with Dependent Variables on Heterogeneous Domains","title":"PDEs with Dependent Variables on Heterogeneous Domains","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL\nimport ModelingToolkit: Interval\n\n@parameters x y\n@variables p(..) q(..) r(..) s(..)\nDx = Differential(x)\nDy = Differential(y)\n\n# 2D PDE\neq = p(x) + q(y) + Dx(r(x, y)) + Dy(s(y, x)) ~ 0\n\n# Initial and boundary conditions\nbcs = [p(1) ~ 0.0f0, q(-1) ~ 0.0f0,\n    r(x, -1) ~ 0.0f0, r(1, y) ~ 0.0f0,\n    s(y, 1) ~ 0.0f0, s(-1, x) ~ 0.0f0]\n\n# Space and time domains\ndomains = [x ∈ Interval(0.0, 1.0),\n    y ∈ Interval(0.0, 1.0)]\n\nnumhid = 3\nchains = [[Lux.Chain(Dense(1, numhid, Lux.σ), Dense(numhid, numhid, Lux.σ),\n                     Dense(numhid, 1)) for i in 1:2]\n          [Lux.Chain(Dense(2, numhid, Lux.σ), Dense(numhid, numhid, Lux.σ),\n                     Dense(numhid, 1)) for i in 1:2]]\ndiscretization = NeuralPDE.PhysicsInformedNN(chains, QuadratureTraining())\n\n@named pde_system = PDESystem(eq, bcs, domains, [x, y], [p(x), q(y), r(x, y), s(y, x)])\nprob = SciMLBase.discretize(pde_system, discretization)\n\ncallback = function (p, l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nres = Optimization.solve(prob, BFGS(); callback = callback, maxiters = 100)","category":"page"},{"location":"examples/ks/#Kuramoto–Sivashinsky-equation","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"","category":"section"},{"location":"examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"Let's consider the Kuramoto–Sivashinsky equation, which contains a 4th-order derivative:","category":"page"},{"location":"examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"_t u(x t) + u(x t) _x u(x t) + alpha ^2_x u(x t) + beta ^3_x u(x t) + gamma ^4_x u(x t) =  0  ","category":"page"},{"location":"examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"where \\alpha = \\gamma = 1 and \\beta = 4. The exact solution is:","category":"page"},{"location":"examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"u_e(x t) = 11 + 15 tanh theta - 15 tanh^2 theta - 15 tanh^3 theta  ","category":"page"},{"location":"examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"where \\theta = t - x/2 and with initial and boundary conditions:","category":"page"},{"location":"examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"beginalign*\n    u(  x 0) =     u_e(  x 0)  \n    u( 10 t) =     u_e( 10 t)  \n    u(-10 t) =     u_e(-10 t)  \n_x u( 10 t) = _x u_e( 10 t)  \n_x u(-10 t) = _x u_e(-10 t)  \nendalign*","category":"page"},{"location":"examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"We use physics-informed neural networks.","category":"page"},{"location":"examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters x, t\n@variables u(..)\nDt = Differential(t)\nDx = Differential(x)\nDx2 = Differential(x)^2\nDx3 = Differential(x)^3\nDx4 = Differential(x)^4\n\nα = 1\nβ = 4\nγ = 1\neq = Dt(u(x, t)) + u(x, t) * Dx(u(x, t)) + α * Dx2(u(x, t)) + β * Dx3(u(x, t)) + γ * Dx4(u(x, t)) ~ 0\n\nu_analytic(x, t; z = -x / 2 + t) = 11 + 15 * tanh(z) - 15 * tanh(z)^2 - 15 * tanh(z)^3\ndu(x, t; z = -x / 2 + t) = 15 / 2 * (tanh(z) + 1) * (3 * tanh(z) - 1) * sech(z)^2\n\nbcs = [u(x, 0) ~ u_analytic(x, 0),\n    u(-10, t) ~ u_analytic(-10, t),\n    u(10, t) ~ u_analytic(10, t),\n    Dx(u(-10, t)) ~ du(-10, t),\n    Dx(u(10, t)) ~ du(10, t)]\n\n# Space and time domains\ndomains = [x ∈ Interval(-10.0, 10.0),\n    t ∈ Interval(0.0, 1.0)]\n# Discretization\ndx = 0.4;\ndt = 0.2;\n\n# Neural network\nchain = Lux.Chain(Dense(2, 12, Lux.σ), Dense(12, 12, Lux.σ), Dense(12, 1))\n\ndiscretization = PhysicsInformedNN(chain, GridTraining([dx, dt]))\n@named pde_system = PDESystem(eq, bcs, domains, [x, t], [u(x, t)])\nprob = discretize(pde_system, discretization)\n\ncallback = function (p, l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nopt = OptimizationOptimJL.BFGS()\nres = Optimization.solve(prob, opt; callback = callback, maxiters = 2000)\nphi = discretization.phi","category":"page"},{"location":"examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"And some analysis:","category":"page"},{"location":"examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"using Plots\n\nxs, ts = [infimum(d.domain):dx:supremum(d.domain)\n          for (d, dx) in zip(domains, [dx / 10, dt])]\n\nu_predict = [[first(phi([x, t], res.u)) for x in xs] for t in ts]\nu_real = [[u_analytic(x, t) for x in xs] for t in ts]\ndiff_u = [[abs(u_analytic(x, t) - first(phi([x, t], res.u))) for x in xs] for t in ts]\n\np1 = plot(xs, u_predict, title = \"predict\")\np2 = plot(xs, u_real, title = \"analytic\")\np3 = plot(xs, diff_u, title = \"error\")\nplot(p1, p2, p3)","category":"page"},{"location":"examples/ks/","page":"Kuramoto–Sivashinsky equation","title":"Kuramoto–Sivashinsky equation","text":"(Image: plotks)","category":"page"},{"location":"examples/nonlinear_hyperbolic/#Nonlinear-hyperbolic-system-of-PDEs","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"","category":"section"},{"location":"examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"We may also solve hyperbolic systems like the following","category":"page"},{"location":"examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"beginaligned\nfracpartial^2upartial t^2 = fracax^n fracpartialpartial x(x^n fracpartial upartial x) + u f(fracuw)  \nfracpartial^2wpartial t^2 = fracbx^n fracpartialpartial x(x^n fracpartial upartial x) + w g(fracuw)  \nendaligned","category":"page"},{"location":"examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"where f and g are arbitrary functions. With initial and boundary conditions:","category":"page"},{"location":"examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"beginaligned\nu(0x) = k * j0(ξ(0 x)) + y0(ξ(0 x)) \nu(t0) = k * j0(ξ(t 0)) + y0(ξ(t 0)) \nu(t1) = k * j0(ξ(t 1)) + y0(ξ(t 1)) \nw(0x) = j0(ξ(0 x)) + y0(ξ(0 x)) \nw(t0) = j0(ξ(t 0)) + y0(ξ(t 0)) \nw(t1) = j0(ξ(t 0)) + y0(ξ(t 0)) \nendaligned","category":"page"},{"location":"examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"where k is a root of the algebraic (transcendental) equation f(k) = g(k), j0 and y0 are the Bessel functions, and ξ(t, x) is:","category":"page"},{"location":"examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"beginaligned\nfracsqrtf(k)sqrtfracax^nsqrtfracax^n(t+1)^2 - (x+1)^2\nendaligned","category":"page"},{"location":"examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"We solve this with Neural:","category":"page"},{"location":"examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL, Roots\nusing SpecialFunctions\nusing Plots\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\n@variables u(..), w(..)\nDx = Differential(x)\nDt = Differential(t)\nDtt = Differential(t)^2\n\n# Constants\na = 16\nb = 16\nn = 0\n\n# Arbitrary functions\nf(x) = x^2\ng(x) = 4 * cos(π * x)\nroot(x) = g(x) - f(x)\n\n# Analytic solution\nk = find_zero(root, (0, 1), Roots.Bisection())                # k is a root of the algebraic (transcendental) equation f(x) = g(x)\nξ(t, x) = sqrt(f(k)) / sqrt(a) * sqrt(a * (t + 1)^2 - (x + 1)^2)\nθ(t, x) = besselj0(ξ(t, x)) + bessely0(ξ(t, x))                     # Analytical solution to Klein-Gordon equation\nw_analytic(t, x) = θ(t, x)\nu_analytic(t, x) = k * θ(t, x)\n\n# Nonlinear system of hyperbolic equations\neqs = [Dtt(u(t, x)) ~ a / (x^n) * Dx(x^n * Dx(u(t, x))) + u(t, x) * f(u(t, x) / w(t, x)),\n    Dtt(w(t, x)) ~ b / (x^n) * Dx(x^n * Dx(w(t, x))) + w(t, x) * g(u(t, x) / w(t, x))]\n\n# Boundary conditions\nbcs = [u(0, x) ~ u_analytic(0, x),\n    w(0, x) ~ w_analytic(0, x),\n    u(t, 0) ~ u_analytic(t, 0),\n    w(t, 0) ~ w_analytic(t, 0),\n    u(t, 1) ~ u_analytic(t, 1),\n    w(t, 1) ~ w_analytic(t, 1)]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0, 1.0),\n    x ∈ Interval(0.0, 1.0)]\n\n# Neural network\ninput_ = length(domains)\nn = 15\nchain = [Lux.Chain(Dense(input_, n, Lux.σ), Dense(n, n, Lux.σ), Dense(n, 1)) for _ in 1:2]\n\nstrategy = QuadratureTraining()\ndiscretization = PhysicsInformedNN(chain, strategy)\n\n@named pdesystem = PDESystem(eqs, bcs, domains, [t, x], [u(t, x), w(t, x)])\nprob = discretize(pdesystem, discretization)\nsym_prob = symbolic_discretize(pdesystem, discretization)\n\npde_inner_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbcs_inner_loss_functions = sym_prob.loss_functions.bc_loss_functions\n\ncallback = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p), bcs_inner_loss_functions))\n    return false\nend\n\nres = Optimization.solve(prob, BFGS(); callback = callback, maxiters = 1000)\n\nphi = discretization.phi\n\n# Analysis\nts, xs = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\ndepvars = [:u, :w]\nminimizers_ = [res.u.depvar[depvars[i]] for i in 1:length(chain)]\n\nanalytic_sol_func(t, x) = [u_analytic(t, x), w_analytic(t, x)]\nu_real = [[analytic_sol_func(t, x)[i] for t in ts for x in xs] for i in 1:2]\nu_predict = [[phi[i]([t, x], minimizers_[i])[1] for t in ts for x in xs] for i in 1:2]\ndiff_u = [abs.(u_real[i] .- u_predict[i]) for i in 1:2]\nfor i in 1:2\n    p1 = plot(ts, xs, u_real[i], linetype = :contourf, title = \"u$i, analytic\")\n    p2 = plot(ts, xs, u_predict[i], linetype = :contourf, title = \"predict\")\n    p3 = plot(ts, xs, diff_u[i], linetype = :contourf, title = \"error\")\n    plot(p1, p2, p3)\n    savefig(\"nonlinear_hyperbolic_sol_u$i\")\nend","category":"page"},{"location":"examples/nonlinear_hyperbolic/","page":"Nonlinear hyperbolic system of PDEs","title":"Nonlinear hyperbolic system of PDEs","text":"(Image: nonlinear_hyperbolic_sol_u1) (Image: nonlinear_hyperbolic_sol_u2)","category":"page"},{"location":"tutorials/derivative_neural_network/#The-Derivative-neural-network-approximation","page":"The Derivative Neural Network Approximation","title":"The Derivative neural network approximation","text":"","category":"section"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"The accuracy and stability of numerical derivative decreases with each successive order. The accuracy of the entire solution is determined by the worst accuracy of one of the variables, in our case, the highest degree of the derivative. Meanwhile, the computational cost of automatic differentiation for higher orders grows as O(n^d), making even numerical differentiation much more efficient! Given these two bad choices, there exists an alternative which can improve training speed and accuracy: using a system to represent the derivatives directly.","category":"page"},{"location":"tutorials/derivative_neural_network/#Demonstration","page":"The Derivative Neural Network Approximation","title":"Demonstration","text":"","category":"section"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"Take the PDE system:","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"beginalign*\n_t^2 u_1(t x)  = _x^2 u_1(t x) + u_3(t x)  sin(pi x)  \n_t^2 u_2(t x)  = _x^2 u_2(t x) + u_3(t x)  cos(pi x)  \n0  = u_1(t x) sin(pi x) + u_2(t x) cos(pi x) - e^-t  \nendalign*","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"with the initial conditions:","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"beginalign*\nu_1(0 x)  = sin(pi x)  \n_t u_1(0 x)  = - sin(pi x)  \nu_2(0 x)  = cos(pi x)  \n_t u_2(0 x)  = - cos(pi x)  \nendalign*","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"and the boundary conditions:","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"beginalign*\nu_1(t 0)  = u_1(t 1) = 0  \nu_2(t 0)  = - u_2(t 1) = e^-t  \nendalign*","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"This is the same system as the system of equations example","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"The derivative neural network approximation is such an approach that using lower-order numeric derivatives and estimates higher-order derivatives with a neural network, so that allows an increase in the marginal precision for all optimization. Since u3 is only in the first and second equations, its accuracy during training is determined by the accuracy of the second numerical derivative u3(t,x) ~ (Dtt(u1(t,x)) -Dxx(u1(t,x))) / sin(pi*x).","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"We approximate the derivative of the neural network with another neural network Dt(u1(t,x)) ~ Dtu1(t,x) and train it along with other equations, and thus we avoid using the second numeric derivative Dt(Dtu1(t,x)).","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"using NeuralPDE, Lux, ModelingToolkit\nusing Optimization, OptimizationOptimisers, OptimizationOptimJL\nusing Plots\nimport ModelingToolkit: Interval, infimum, supremum\n\n@parameters t, x\nDt = Differential(t)\nDx = Differential(x)\n@variables u1(..), u2(..), u3(..)\n@variables Dxu1(..) Dtu1(..) Dxu2(..) Dtu2(..)\n\neqs_ = [Dt(Dtu1(t, x)) ~ Dx(Dxu1(t, x)) + u3(t, x) * sin(pi * x),\n    Dt(Dtu2(t, x)) ~ Dx(Dxu2(t, x)) + u3(t, x) * cos(pi * x),\n    exp(-t) ~ u1(t, x) * sin(pi * x) + u2(t, x) * cos(pi * x)]\n\nbcs_ = [u1(0.0, x) ~ sin(pi * x),\n    u2(0.0, x) ~ cos(pi * x),\n    Dt(u1(0, x)) ~ -sin(pi * x),\n    Dt(u2(0, x)) ~ -cos(pi * x),\n    #Dtu1(0,x) ~ -sin(pi*x),\n    # Dtu2(0,x) ~ -cos(pi*x),\n    u1(t, 0.0) ~ 0.0,\n    u2(t, 0.0) ~ exp(-t),\n    u1(t, 1.0) ~ 0.0,\n    u2(t, 1.0) ~ -exp(-t)]\n\nder_ = [Dt(u1(t, x)) ~ Dtu1(t, x),\n    Dt(u2(t, x)) ~ Dtu2(t, x),\n    Dx(u1(t, x)) ~ Dxu1(t, x),\n    Dx(u2(t, x)) ~ Dxu2(t, x)]\n\nbcs__ = [bcs_; der_]\n\n# Space and time domains\ndomains = [t ∈ Interval(0.0, 1.0),\n    x ∈ Interval(0.0, 1.0)]\n\ninput_ = length(domains)\nn = 15\nchain = [Lux.Chain(Dense(input_, n, Lux.σ), Dense(n, n, Lux.σ), Dense(n, 1)) for _ in 1:7]\n\ngrid_strategy = NeuralPDE.GridTraining(0.07)\ndiscretization = NeuralPDE.PhysicsInformedNN(chain,\n                                             grid_strategy)\n\nvars = [u1(t, x), u2(t, x), u3(t, x), Dxu1(t, x), Dtu1(t, x), Dxu2(t, x), Dtu2(t, x)]\n@named pdesystem = PDESystem(eqs_, bcs__, domains, [t, x], vars)\nprob = NeuralPDE.discretize(pdesystem, discretization)\nsym_prob = NeuralPDE.symbolic_discretize(pdesystem, discretization)\n\npde_inner_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbcs_inner_loss_functions = sym_prob.loss_functions.bc_loss_functions[1:7]\naprox_derivative_loss_functions = sym_prob.loss_functions.bc_loss_functions[9:end]\n\ncallback = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p), bcs_inner_loss_functions))\n    println(\"der_losses: \", map(l_ -> l_(p), aprox_derivative_loss_functions))\n    return false\nend\n\nres = Optimization.solve(prob, Adam(0.01); callback = callback, maxiters = 2000)\nprob = remake(prob, u0 = res.u)\nres = Optimization.solve(prob, BFGS(); callback = callback, maxiters = 10000)\n\nphi = discretization.phi","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"And some analysis:","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"using Plots\n\nts, xs = [infimum(d.domain):0.01:supremum(d.domain) for d in domains]\nminimizers_ = [res.u.depvar[sym_prob.depvars[i]] for i in 1:length(chain)]\n\nu1_real(t, x) = exp(-t) * sin(pi * x)\nu2_real(t, x) = exp(-t) * cos(pi * x)\nu3_real(t, x) = (1 + pi^2) * exp(-t)\nDxu1_real(t, x) = pi * exp(-t) * cos(pi * x)\nDtu1_real(t, x) = -exp(-t) * sin(pi * x)\nDxu2_real(t, x) = -pi * exp(-t) * sin(pi * x)\nDtu2_real(t, x) = -exp(-t) * cos(pi * x)\nfunction analytic_sol_func_all(t, x)\n    [u1_real(t, x), u2_real(t, x), u3_real(t, x),\n        Dxu1_real(t, x), Dtu1_real(t, x), Dxu2_real(t, x), Dtu2_real(t, x)]\nend\n\nu_real = [[analytic_sol_func_all(t, x)[i] for t in ts for x in xs] for i in 1:7]\nu_predict = [[phi[i]([t, x], minimizers_[i])[1] for t in ts for x in xs] for i in 1:7]\ndiff_u = [abs.(u_real[i] .- u_predict[i]) for i in 1:7]\n\ntitles = [\"u1\", \"u2\", \"u3\", \"Dtu1\", \"Dtu2\", \"Dxu1\", \"Dxu2\"]\nfor i in 1:7\n    p1 = plot(ts, xs, u_real[i], linetype = :contourf, title = \"$(titles[i]), analytic\")\n    p2 = plot(ts, xs, u_predict[i], linetype = :contourf, title = \"predict\")\n    p3 = plot(ts, xs, diff_u[i], linetype = :contourf, title = \"error\")\n    plot(p1, p2, p3)\n    savefig(\"3sol_ub$i\")\nend","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"(Image: aprNN_sol_u1)","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"(Image: aprNN_sol_u2)","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"(Image: aprNN_sol_u3)","category":"page"},{"location":"tutorials/derivative_neural_network/#Comparison-of-the-second-numerical-derivative-and-numerical-neural-network-derivative","page":"The Derivative Neural Network Approximation","title":"Comparison of the second numerical derivative and numerical + neural network derivative","text":"","category":"section"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"(Image: DDu1)","category":"page"},{"location":"tutorials/derivative_neural_network/","page":"The Derivative Neural Network Approximation","title":"The Derivative Neural Network Approximation","text":"(Image: DDu2)","category":"page"},{"location":"tutorials/pdesystem/#Specifying-and-Solving-PDESystems-with-Physics-Informed-Neural-Networks-(PINNs)","page":"Introduction to NeuralPDE for PDEs","title":"Specifying and Solving PDESystems with Physics-Informed Neural Networks (PINNs)","text":"","category":"section"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"In this example, we will solve a Poisson equation:","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"^2_x u(x y) + ^2_y u(x y) = - sin(pi x) sin(pi y)  ","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"with the boundary conditions:","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"beginalign*\nu(0 y) = 0  \nu(1 y) = 0  \nu(x 0) = 0  \nu(x 1) = 0  \nendalign*","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"on the space domain:","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"x in 0 1    y in 0 1  ","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"with grid discretization dx = 0.05 using physics-informed neural networks.","category":"page"},{"location":"tutorials/pdesystem/#Copy-Pasteable-Code","page":"Introduction to NeuralPDE for PDEs","title":"Copy-Pasteable Code","text":"","category":"section"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"using NeuralPDE, Lux, Optimization, OptimizationOptimJL\nimport ModelingToolkit: Interval\n\n@parameters x y\n@variables u(..)\nDxx = Differential(x)^2\nDyy = Differential(y)^2\n\n# 2D PDE\neq = Dxx(u(x, y)) + Dyy(u(x, y)) ~ -sin(pi * x) * sin(pi * y)\n\n# Boundary conditions\nbcs = [u(0, y) ~ 0.0, u(1, y) ~ 0.0,\n    u(x, 0) ~ 0.0, u(x, 1) ~ 0.0]\n# Space and time domains\ndomains = [x ∈ Interval(0.0, 1.0),\n    y ∈ Interval(0.0, 1.0)]\n\n# Neural network\ndim = 2 # number of dimensions\nchain = Lux.Chain(Dense(dim, 16, Lux.σ), Dense(16, 16, Lux.σ), Dense(16, 1))\n\n# Discretization\ndx = 0.05\ndiscretization = PhysicsInformedNN(chain, GridTraining(dx))\n\n@named pde_system = PDESystem(eq, bcs, domains, [x, y], [u(x, y)])\nprob = discretize(pde_system, discretization)\n\n#Optimizer\nopt = OptimizationOptimJL.BFGS()\n\n#Callback function\ncallback = function (p, l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nres = Optimization.solve(prob, opt, callback = callback, maxiters = 1000)\nphi = discretization.phi\n\nusing Plots\n\nxs, ys = [infimum(d.domain):(dx / 10):supremum(d.domain) for d in domains]\nanalytic_sol_func(x, y) = (sin(pi * x) * sin(pi * y)) / (2pi^2)\n\nu_predict = reshape([first(phi([x, y], res.u)) for x in xs for y in ys],\n                    (length(xs), length(ys)))\nu_real = reshape([analytic_sol_func(x, y) for x in xs for y in ys],\n                 (length(xs), length(ys)))\ndiff_u = abs.(u_predict .- u_real)\n\np1 = plot(xs, ys, u_real, linetype = :contourf, title = \"analytic\");\np2 = plot(xs, ys, u_predict, linetype = :contourf, title = \"predict\");\np3 = plot(xs, ys, diff_u, linetype = :contourf, title = \"error\");\nplot(p1, p2, p3)","category":"page"},{"location":"tutorials/pdesystem/#Detailed-Description","page":"Introduction to NeuralPDE for PDEs","title":"Detailed Description","text":"","category":"section"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"The ModelingToolkit PDE interface for this example looks like this:","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL\nimport ModelingToolkit: Interval\n\n@parameters x y\n@variables u(..)\n@derivatives Dxx'' ~ x\n@derivatives Dyy'' ~ y\n\n# 2D PDE\neq = Dxx(u(x, y)) + Dyy(u(x, y)) ~ -sin(pi * x) * sin(pi * y)\n\n# Boundary conditions\nbcs = [u(0, y) ~ 0.0, u(1, y) ~ 0.0,\n    u(x, 0) ~ 0.0, u(x, 1) ~ 0.0]\n# Space and time domains\ndomains = [x ∈ Interval(0.0, 1.0),\n    y ∈ Interval(0.0, 1.0)]","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"Here, we define the neural network, where the input of NN equals the number of dimensions and output equals the number of equations in the system.","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"# Neural network\ndim = 2 # number of dimensions\nchain = Lux.Chain(Dense(dim, 16, Lux.σ), Dense(16, 16, Lux.σ), Dense(16, 1))","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"Here, we build PhysicsInformedNN algorithm where dx is the step of discretization where strategy stores information for choosing a training strategy.","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"# Discretization\ndx = 0.05\ndiscretization = PhysicsInformedNN(chain, GridTraining(dx))","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"As described in the API docs, we now need to define the PDESystem and create PINNs problem using the discretize method.","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"@named pde_system = PDESystem(eq, bcs, domains, [x, y], [u(x, y)])\nprob = discretize(pde_system, discretization)","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"Here, we define the callback function and the optimizer. And now we can solve the PDE using PINNs (with the number of epochs maxiters=1000).","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"#Optimizer\nopt = OptimizationOptimJL.BFGS()\n\ncallback = function (p, l)\n    println(\"Current loss is: $l\")\n    return false\nend\n\nres = Optimization.solve(prob, opt, callback = callback, maxiters = 1000)\nphi = discretization.phi","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"We can plot the predicted solution of the PDE and compare it with the analytical solution to plot the relative error.","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"xs, ys = [infimum(d.domain):(dx / 10):supremum(d.domain) for d in domains]\nanalytic_sol_func(x, y) = (sin(pi * x) * sin(pi * y)) / (2pi^2)\n\nu_predict = reshape([first(phi([x, y], res.u)) for x in xs for y in ys],\n                    (length(xs), length(ys)))\nu_real = reshape([analytic_sol_func(x, y) for x in xs for y in ys],\n                 (length(xs), length(ys)))\ndiff_u = abs.(u_predict .- u_real)\n\nusing Plots\n\np1 = plot(xs, ys, u_real, linetype = :contourf, title = \"analytic\");\np2 = plot(xs, ys, u_predict, linetype = :contourf, title = \"predict\");\np3 = plot(xs, ys, diff_u, linetype = :contourf, title = \"error\");\nplot(p1, p2, p3)","category":"page"},{"location":"tutorials/pdesystem/","page":"Introduction to NeuralPDE for PDEs","title":"Introduction to NeuralPDE for PDEs","text":"(Image: poissonplot)","category":"page"},{"location":"manual/adaptive_losses/#adaptive_loss","page":"Adaptive Loss Functions","title":"Adaptive Loss Functions","text":"","category":"section"},{"location":"manual/adaptive_losses/","page":"Adaptive Loss Functions","title":"Adaptive Loss Functions","text":"The NeuralPDE discretize function allows for specifying adaptive loss function strategy which improve training performance by reweighing the equations as necessary to ensure the boundary conditions are well-satisfied, even in ill-conditioned scenarios. The following are the options for the adaptive_loss:","category":"page"},{"location":"manual/adaptive_losses/","page":"Adaptive Loss Functions","title":"Adaptive Loss Functions","text":"NeuralPDE.NonAdaptiveLoss\nNeuralPDE.GradientScaleAdaptiveLoss\nNeuralPDE.MiniMaxAdaptiveLoss","category":"page"},{"location":"manual/adaptive_losses/#NeuralPDE.NonAdaptiveLoss","page":"Adaptive Loss Functions","title":"NeuralPDE.NonAdaptiveLoss","text":"NonAdaptiveLoss{T}(; pde_loss_weights = 1,\n                     bc_loss_weights = 1,\n                     additional_loss_weights = 1)\n\nA way of loss weighting the components of the loss function in the total sum that does not change during optimization\n\n\n\n\n\n","category":"type"},{"location":"manual/adaptive_losses/#NeuralPDE.GradientScaleAdaptiveLoss","page":"Adaptive Loss Functions","title":"NeuralPDE.GradientScaleAdaptiveLoss","text":"GradientScaleAdaptiveLoss(reweight_every;\n                          weight_change_inertia = 0.9,\n                          pde_loss_weights = 1,\n                          bc_loss_weights = 1,\n                          additional_loss_weights = 1)\n\nA way of adaptively reweighting the components of the loss function in the total sum such that BCi loss weights are scaled by the exponential moving average of max(|∇pdeloss|)/mean(|∇bciloss|) )\n\nPositional Arguments\n\nreweight_every: how often to reweight the BC loss functions, measured in iterations. Reweighting is somewhat expensive since it involves evaluating the gradient of each component loss function,\n\nKeyword Arguments\n\nweight_change_inertia: a real number that represents the inertia of the exponential moving average of the BC weight changes,\n\nReferences\n\nUnderstanding and mitigating gradient pathologies in physics-informed neural networks Sifan Wang, Yujun Teng, Paris Perdikaris https://arxiv.org/abs/2001.04536v1\n\nWith code reference: https://github.com/PredictiveIntelligenceLab/GradientPathologiesPINNs\n\n\n\n\n\n","category":"type"},{"location":"manual/adaptive_losses/#NeuralPDE.MiniMaxAdaptiveLoss","page":"Adaptive Loss Functions","title":"NeuralPDE.MiniMaxAdaptiveLoss","text":"function MiniMaxAdaptiveLoss(reweight_every;\n                             pde_max_optimiser = Flux.ADAM(1e-4),\n                             bc_max_optimiser = Flux.ADAM(0.5),\n                             pde_loss_weights = 1,\n                             bc_loss_weights = 1,\n                             additional_loss_weights = 1)\n\nA way of adaptively reweighting the components of the loss function in the total sum such that the loss weights are maximized by an internal optimizer, which leads to a behavior where loss functions that have not been satisfied get a greater weight,\n\nPositional Arguments\n\nreweight_every: how often to reweight the PDE and BC loss functions, measured in iterations.  Reweighting is cheap since it re-uses the value of loss functions generated during the main optimization loop.\n\nKeyword Arguments\n\npde_max_optimiser: a Flux.Optimise.AbstractOptimiser that is used internally to maximize the weights of the PDE loss functions.\nbc_max_optimiser: a Flux.Optimise.AbstractOptimiser that is used internally to maximize the weights of the BC loss functions.\n\nReferences\n\nSelf-Adaptive Physics-Informed Neural Networks using a Soft Attention Mechanism Levi McClenny, Ulisses Braga-Neto https://arxiv.org/abs/2009.04544\n\n\n\n\n\n","category":"type"},{"location":"manual/nnrode/#Random-Ordinary-Differential-Equation-Specialized-Physics-Informed-Neural-Solver","page":"Random Ordinary Differential Equation Specialized Physics-Informed Neural Solver","title":"Random Ordinary Differential Equation Specialized Physics-Informed Neural Solver","text":"","category":"section"},{"location":"manual/nnrode/","page":"Random Ordinary Differential Equation Specialized Physics-Informed Neural Solver","title":"Random Ordinary Differential Equation Specialized Physics-Informed Neural Solver","text":"TODO","category":"page"},{"location":"#NeuralPDE.jl:-Automatic-Physics-Informed-Neural-Networks-(PINNs)","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"","category":"section"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"NeuralPDE.jl is a solver package which  consists of neural network solvers for partial differential equations using  physics-informed neural networks (PINNs) and the ability to generate neural networks which both approximate physical laws and real data simultaniously.","category":"page"},{"location":"#Features","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"Features","text":"","category":"section"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"Physics-Informed Neural Networks for ODE, SDE, RODE, and PDE solving.\nAbility to define extra loss functions to mix xDE solving with data fitting (scientific machine learning).\nAutomated construction of Physics-Informed loss functions from a high-level symbolic interface.\nSophisticated techniques like quadrature training strategies, adaptive loss functions, and neural adapters to accelerate training.\nIntegrated logging suite for handling connections to TensorBoard.\nHandling of (partial) integro-differential equations and various stochastic equations.\nSpecialized forms for solving ODEProblems with neural networks.\nCompatibility with Flux.jl and Lux.jl. for all the GPU-powered machine learning layers available from those libraries.\nCompatibility with NeuralOperators.jl for mixing DeepONets and other neural operators (Fourier Neural Operators, Graph Neural Operators, etc.) with physics-informed loss functions.","category":"page"},{"location":"#Installation","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"Installation","text":"","category":"section"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"Assuming that you already have Julia correctly installed, it suffices to import NeuralPDE.jl in the standard way:","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"import Pkg\nPkg.add(\"NeuralPDE\")","category":"page"},{"location":"#Contributing","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"Contributing","text":"","category":"section"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"Please refer to the SciML ColPrac: Contributor's Guide on Collaborative Practices for Community Packages for guidance on PRs, issues, and other matters relating to contributing to SciML.\nSee the SciML Style Guide for common coding practices and other style decisions.\nThere are a few community forums:\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Slack\nThe #diffeq-bridged and #sciml-bridged channels in the Julia Zulip\nOn the Julia Discourse forums\nSee also SciML Community page","category":"page"},{"location":"#Citation","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"Citation","text":"","category":"section"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"If you use NeuralPDE.jl in your research, please cite this paper:","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"@misc{https://doi.org/10.48550/arxiv.2107.09443,\n  doi = {10.48550/ARXIV.2107.09443},\n  url = {https://arxiv.org/abs/2107.09443},\n  author = {Zubov, Kirill and McCarthy, Zoe and Ma, Yingbo and Calisto, Francesco and Pagliarino, Valerio and Azeglio, Simone and Bottero, Luca and Luján, Emmanuel and Sulzer, Valentin and Bharambe, Ashutosh and Vinchhi, Nand and Balakrishnan, Kaushik and Upadhyay, Devesh and Rackauckas, Chris},\n  keywords = {Mathematical Software (cs.MS), Symbolic Computation (cs.SC), FOS: Computer and information sciences, FOS: Computer and information sciences},\n  title = {NeuralPDE: Automating Physics-Informed Neural Networks (PINNs) with Error Approximations},\n  publisher = {arXiv},\n  year = {2021},\n  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}\n}","category":"page"},{"location":"#Flux.jl-vs-Lux.jl","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"Flux.jl vs Lux.jl","text":"","category":"section"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"Both Flux and Lux defined neural networks are supported by NeuralPDE.jl. However, Lux.jl neural networks are greatly preferred for many correctness reasons. Particularly, a Flux Chain does not respect Julia's type promotion rules. This causes major problems in that the restructuring of a Flux neural network will not respect the chosen types from the solver. Demonstration:","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"using Flux, Tracker\nx = [0.8; 0.8]\nann = Chain(Dense(2, 10, tanh), Dense(10, 1))\np, re = Flux.destructure(ann)\nz = re(Float64(p))","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"While one may think this recreates the neural network to act in Float64 precision, it does not and instead its values will silently downgrade everything to Float32. This is only fixed by Chain(Dense(2, 10, tanh), Dense(10, 1)) |> f64. Similar cases will lead to dropped gradients with complex numbers. This is not an issue with the automatic differentiation library commonly associated with Flux (Zygote.jl) but rather due to choices in the neural network library's decision for how to approach type handling and precision. Thus when using DiffEqFlux.jl with Flux, the user must be very careful to ensure that the precision of the arguments are correct, and anything that requires alternative types (like TrackerAdjoint tracked values, ForwardDiffSensitivity dual numbers, and TaylorDiff.jl differentiation) are suspect.","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"Lux.jl has none of these issues, is simpler to work with due to the parameters in its function calls being explicit rather than implicit global references, and achieves higher performance. It is built on the same foundations as Flux.jl, such as Zygote and NNLib, and thus it supports the same layers underneith and calls the same kernels. The better performance comes from not having the overhead of restructure required. Thus we highly recommend people use Lux instead and only use the Flux fallbacks for legacy code.","category":"page"},{"location":"#Reproducibility","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"Reproducibility","text":"","category":"section"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"<details><summary>The documentation of this SciML package was built using these direct dependencies,</summary>","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"using Pkg # hide\nPkg.status() # hide","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"</details>","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"<details><summary>and using this machine and Julia version.</summary>","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"using InteractiveUtils # hide\nversioninfo() # hide","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"</details>","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"<details><summary>A more complete overview of all dependencies and their versions is also provided.</summary>","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"using Pkg # hide\nPkg.status(; mode = PKGMODE_MANIFEST) # hide","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"</details>","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"You can also download the \n<a href=\"","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"using TOML\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n       \"/assets/Manifest.toml\"","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"\">manifest</a> file and the\n<a href=\"","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"using TOML\nversion = TOML.parse(read(\"../../Project.toml\", String))[\"version\"]\nname = TOML.parse(read(\"../../Project.toml\", String))[\"name\"]\nlink = \"https://github.com/SciML/\" * name * \".jl/tree/gh-pages/v\" * version *\n       \"/assets/Project.toml\"","category":"page"},{"location":"","page":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","title":"NeuralPDE.jl: Automatic Physics-Informed Neural Networks (PINNs)","text":"\">project</a> file.","category":"page"},{"location":"tutorials/constraints/#Imposing-Constraints-on-Physics-Informed-Neural-Network-(PINN)-Solutions","page":"Imposing Constraints","title":"Imposing Constraints on Physics-Informed Neural Network (PINN) Solutions","text":"","category":"section"},{"location":"tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"Let's consider the Fokker-Planck equation:","category":"page"},{"location":"tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"- fracx left  left( alpha x - beta x^3right) p(x)right  + fracsigma^22 frac^2x^2 p(x) = 0  ","category":"page"},{"location":"tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"which must satisfy the normalization condition:","category":"page"},{"location":"tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"Delta t  p(x) = 1","category":"page"},{"location":"tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"with the boundary conditions:","category":"page"},{"location":"tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"p(-22) = p(22) = 0","category":"page"},{"location":"tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"with Physics-Informed Neural Networks.","category":"page"},{"location":"tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL\nusing Integrals, IntegralsCubature\nimport ModelingToolkit: Interval, infimum, supremum\n# the example is taken from this article https://arxiv.org/abs/1910.10503\n@parameters x\n@variables p(..)\nDx = Differential(x)\nDxx = Differential(x)^2\n\nα = 0.3\nβ = 0.5\n_σ = 0.5\nx_0 = -2.2\nx_end = 2.2\n# Discretization\ndx = 0.01\n\neq = Dx((α * x - β * x^3) * p(x)) ~ (_σ^2 / 2) * Dxx(p(x))\n\n# Initial and boundary conditions\nbcs = [p(x_0) ~ 0.0, p(x_end) ~ 0.0]\n\n# Space and time domains\ndomains = [x ∈ Interval(x_0, x_end)]\n\n# Neural network\ninn = 18\nchain = Lux.Chain(Dense(1, inn, Lux.σ),\n                  Dense(inn, inn, Lux.σ),\n                  Dense(inn, inn, Lux.σ),\n                  Dense(inn, 1))\n\nlb = [x_0]\nub = [x_end]\nfunction norm_loss_function(phi, θ, p)\n    function inner_f(x, θ)\n        dx * phi(x, θ) .- 1\n    end\n    prob = IntegralProblem(inner_f, lb, ub, θ)\n    norm2 = solve(prob, HCubatureJL(), reltol = 1e-8, abstol = 1e-8, maxiters = 10)\n    abs(norm2[1])\nend\n\ndiscretization = PhysicsInformedNN(chain,\n                                   GridTraining(dx),\n                                   additional_loss = norm_loss_function)\n\n@named pdesystem = PDESystem(eq, bcs, domains, [x], [p(x)])\nprob = discretize(pdesystem, discretization)\nphi = discretization.phi\n\nsym_prob = NeuralPDE.symbolic_discretize(pdesystem, discretization)\n\npde_inner_loss_functions = sym_prob.loss_functions.pde_loss_functions\nbcs_inner_loss_functions = sym_prob.loss_functions.bc_loss_functions\naprox_derivative_loss_functions = sym_prob.loss_functions.bc_loss_functions\n\ncb_ = function (p, l)\n    println(\"loss: \", l)\n    println(\"pde_losses: \", map(l_ -> l_(p), pde_inner_loss_functions))\n    println(\"bcs_losses: \", map(l_ -> l_(p), bcs_inner_loss_functions))\n    println(\"additional_loss: \", norm_loss_function(phi, p, nothing))\n    return false\nend\n\nres = Optimization.solve(prob, LBFGS(), callback = cb_, maxiters = 400)\nprob = remake(prob, u0 = res.u)\nres = Optimization.solve(prob, BFGS(), callback = cb_, maxiters = 2000)","category":"page"},{"location":"tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"And some analysis:","category":"page"},{"location":"tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"using Plots\nC = 142.88418699042 #fitting param\nanalytic_sol_func(x) = C * exp((1 / (2 * _σ^2)) * (2 * α * x^2 - β * x^4))\n\nxs = [infimum(d.domain):dx:supremum(d.domain) for d in domains][1]\nu_real = [analytic_sol_func(x) for x in xs]\nu_predict = [first(phi(x, res.u)) for x in xs]\n\nplot(xs, u_real, label = \"analytic\")\nplot!(xs, u_predict, label = \"predict\")","category":"page"},{"location":"tutorials/constraints/","page":"Imposing Constraints","title":"Imposing Constraints","text":"(Image: fp)","category":"page"},{"location":"developer/debugging/#Debugging-PINN-Solutions","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"","category":"section"},{"location":"developer/debugging/#Note-this-is-all-not-current-right-now!","page":"Debugging PINN Solutions","title":"Note this is all not current right now!","text":"","category":"section"},{"location":"developer/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"Let's walk through debugging functions for the physics-informed neural network PDE solvers.","category":"page"},{"location":"developer/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"using NeuralPDE, ModelingToolkit, Flux, Zygote\nimport ModelingToolkit: Interval, infimum, supremum\n# 2d wave equation, neumann boundary condition\n@parameters x, t\n@variables u(..)\nDxx = Differential(x)^2\nDtt = Differential(t)^2\nDt = Differential(t)\n#2D PDE\nC = 1\neq = Dtt(u(x, t)) ~ C^2 * Dxx(u(x, t))\n\n# Initial and boundary conditions\nbcs = [u(0, t) ~ 0.0,\n    u(1, t) ~ 0.0,\n    u(x, 0) ~ x * (1.0 - x),\n    Dt(u(x, 0)) ~ 0.0]\n\n# Space and time domains\ndomains = [x ∈ Interval(0.0, 1.0),\n    t ∈ Interval(0.0, 1.0)]\n\n# Neural network\nchain = FastChain(FastDense(2, 16, Flux.σ), FastDense(16, 16, Flux.σ), FastDense(16, 1))\ninit_params = DiffEqFlux.initial_params(chain)\n\neltypeθ = eltype(init_params)\nphi = NeuralPDE.get_phi(chain)\nderivative = NeuralPDE.get_numeric_derivative()\n\nu_ = (cord, θ, phi) -> sum(phi(cord, θ))\n\nphi([1, 2], init_params)\n\nphi_ = (p) -> phi(p, init_params)[1]\ndphi = Zygote.gradient(phi_, [1.0, 2.0])\n\ndphi1 = derivative(phi, u_, [1.0, 2.0], [[0.0049215667, 0.0]], 1, init_params)\ndphi2 = derivative(phi, u_, [1.0, 2.0], [[0.0, 0.0049215667]], 1, init_params)\nisapprox(dphi[1][1], dphi1, atol = 1e-8)\nisapprox(dphi[1][2], dphi2, atol = 1e-8)\n\nindvars = [x, t]\ndepvars = [u(x, t)]\ndict_depvars_input = Dict(:u => [:x, :t])\ndim = length(domains)\ndx = 0.1\nmultioutput = chain isa AbstractArray\nstrategy = NeuralPDE.GridTraining(dx)\nintegral = NeuralPDE.get_numeric_integral(strategy, indvars, multioutput, chain, derivative)\n\n_pde_loss_function = NeuralPDE.build_loss_function(eq, indvars, depvars, phi, derivative,\n                                                   integral, multioutput, init_params,\n                                                   strategy)","category":"page"},{"location":"developer/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"julia> expr_pde_loss_function = NeuralPDE.build_symbolic_loss_function(eq,indvars,depvars,dict_depvars_input,phi,derivative,integral,multioutput,init_params,strategy)\n\n:((cord, var\"##θ#529\", phi, derivative, integral, u)->begin\n          begin\n              let (x, t) = (cord[[1], :], cord[[2], :])\n                  derivative.(phi, u, cord, Array{Float32,1}[[0.0, 0.0049215667], [0.0, 0.0049215667]], 2, var\"##θ#529\") .- derivative.(phi, u, cord, Array{Float32,1}[[0.0049215667, 0.0], [0.0049215667, 0.0]], 2, var\"##θ#529\")\n              end\n          end\n      end)\n\njulia> bc_indvars = NeuralPDE.get_variables(bcs,indvars,depvars)\n4-element Array{Array{Any,1},1}:\n [:t]\n [:t]\n [:x]\n [:x]","category":"page"},{"location":"developer/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"_bc_loss_functions = [NeuralPDE.build_loss_function(bc, indvars, depvars,\n                                                    phi, derivative, integral, multioutput,\n                                                    init_params, strategy,\n                                                    bc_indvars = bc_indvar)\n                      for (bc, bc_indvar) in zip(bcs, bc_indvars)]","category":"page"},{"location":"developer/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"julia> expr_bc_loss_functions = [NeuralPDE.build_symbolic_loss_function(bc,indvars,depvars,dict_depvars_input,\n                                                                        phi,derivative,integral,multioutput,init_params,strategy,\n                                                                        bc_indvars = bc_indvar) for (bc,bc_indvar) in zip(bcs,bc_indvars)]\n4-element Array{Expr,1}:\n :((cord, var\"##θ#529\", phi, derivative, integral, u)->begin\n          begin\n              let (x, t) = (cord[[1], :], cord[[2], :])\n                  u.(cord, var\"##θ#529\", phi) .- 0.0\n              end\n          end\n      end)\n :((cord, var\"##θ#529\", phi, derivative, integral, u)->begin\n          begin\n              let (x, t) = (cord[[1], :], cord[[2], :])\n                  u.(cord, var\"##θ#529\", phi) .- 0.0\n              end\n          end\n      end)\n :((cord, var\"##θ#529\", phi, derivative, integral, u)->begin\n          begin\n              let (x, t) = (cord[[1], :], cord[[2], :])\n                  u.(cord, var\"##θ#529\", phi) .- (*).(x, (+).(1.0, (*).(-1, x)))\n              end\n          end\n      end)\n :((cord, var\"##θ#529\", phi, derivative, integral, u)->begin\n          begin\n              let (x, t) = (cord[[1], :], cord[[2], :])\n                  derivative.(phi, u, cord, Array{Float32,1}[[0.0, 0.0049215667]], 1, var\"##θ#529\") .- 0.0\n              end\n          end\n      end)","category":"page"},{"location":"developer/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"train_sets = NeuralPDE.generate_training_sets(domains, dx, [eq], bcs, eltypeθ, indvars,\n                                              depvars)\npde_train_set, bcs_train_set = train_sets","category":"page"},{"location":"developer/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"julia> pde_train_set\n1-element Array{Array{Float32,2},1}:\n [0.1 0.2 … 0.8 0.9; 0.1 0.1 … 1.0 1.0]\n\n\njulia> bcs_train_set\n4-element Array{Array{Float32,2},1}:\n [0.0 0.0 … 0.0 0.0; 0.0 0.1 … 0.9 1.0]\n [1.0 1.0 … 1.0 1.0; 0.0 0.1 … 0.9 1.0]\n [0.0 0.1 … 0.9 1.0; 0.0 0.0 … 0.0 0.0]\n [0.0 0.1 … 0.9 1.0; 0.0 0.0 … 0.0 0.0]","category":"page"},{"location":"developer/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"pde_bounds, bcs_bounds = NeuralPDE.get_bounds(domains, [eq], bcs, eltypeθ, indvars, depvars,\n                                              NeuralPDE.StochasticTraining(100))","category":"page"},{"location":"developer/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"julia> pde_bounds\n1-element Vector{Vector{Any}}:\n [Float32[0.01, 0.99], Float32[0.01, 0.99]]\n\njulia> bcs_bounds\n4-element Vector{Vector{Any}}:\n [0, Float32[0.0, 1.0]]\n [1, Float32[0.0, 1.0]]\n [Float32[0.0, 1.0], 0]\n [Float32[0.0, 1.0], 0]","category":"page"},{"location":"developer/debugging/","page":"Debugging PINN Solutions","title":"Debugging PINN Solutions","text":"discretization = NeuralPDE.PhysicsInformedNN(chain, strategy)\n\n@named pde_system = PDESystem(eq, bcs, domains, indvars, depvars)\nprob = NeuralPDE.discretize(pde_system, discretization)\n\nexpr_prob = NeuralPDE.symbolic_discretize(pde_system, discretization)\nexpr_pde_loss_function, expr_bc_loss_functions = expr_prob","category":"page"}]
}
